{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of main_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "interpreter": {
      "hash": "d2c77ea8d338d5dbf262d683b98c2873f87d453475070b582f093a0c2749b897"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjdvyD62otP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e3702e-4239-4cb6-e323-f02908735be9"
      },
      "source": [
        "!pip install git+https://github.com/PennyLaneAI/pennylane\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PennyLaneAI/pennylane\n",
            "  Cloning https://github.com/PennyLaneAI/pennylane to /tmp/pip-req-build-6b1g3ckv\n",
            "  Running command git clone -q https://github.com/PennyLaneAI/pennylane /tmp/pip-req-build-6b1g3ckv\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (2.6.3)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.4.4)\n",
            "Collecting semantic_version==2.6\n",
            "  Downloading semantic_version-2.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting autoray\n",
            "  Downloading autoray-0.2.5-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (4.2.2)\n",
            "Collecting pennylane-lightning>=0.18\n",
            "  Downloading PennyLane_Lightning-0.18.0-cp37-cp37m-manylinux2010_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd->PennyLane==0.19.0.dev0) (0.16.0)\n",
            "Building wheels for collected packages: PennyLane\n",
            "  Building wheel for PennyLane (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PennyLane: filename=PennyLane-0.19.0.dev0-py3-none-any.whl size=650237 sha256=0b001d28277354c612b1ac63f95e4504982e779f820bf088e9e4fb61d25727f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ig32yx2d/wheels/9e/4b/fe/27dcf8ba174161f9d1af1841251dc97013a33a66f16cd8d661\n",
            "Successfully built PennyLane\n",
            "Installing collected packages: semantic-version, pennylane-lightning, autoray, PennyLane\n",
            "Successfully installed PennyLane-0.19.0.dev0 autoray-0.2.5 pennylane-lightning-0.18.0 semantic-version-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z70RfQb52cPX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.templates import embeddings as emb\n",
        "from pennylane.templates import layers as lay\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "import glob, pickle, time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y11nHDFJ79-C",
        "outputId": "9ac4bf6d-3d29-41dc-8c83-6ebdcd48f6ec"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "uKEIMOeK4VQV",
        "outputId": "c569b4a7-046e-4f4a-f827-8a6f62016471"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "uploaded.keys()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd491d1a-cfdb-4723-b25f-0ae987d1c4e1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd491d1a-cfdb-4723-b25f-0ae987d1c4e1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving notes.pk to notes.pk\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['notes.pk'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyTkc6uS2cPb"
      },
      "source": [
        "# Midi.py\n",
        "\n",
        "notes_dir = \"notes.pk\"\n",
        "\n",
        "class Midi:\n",
        "    def __init__(self, seq_length, device):\n",
        "        self.seq_length = seq_length\n",
        "        self.device = device\n",
        "\n",
        "        if Path(notes_dir).is_file():\n",
        "            self.notes = pickle.load(open(notes_dir, \"rb\"))\n",
        "            # self.notes = pickle.loads(uploaded[notes_dir])\n",
        "        else:\n",
        "            self.notes = self.get_notes()\n",
        "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\n",
        "\n",
        "        self.network_input, self.network_output = self.prepare_sequences(self.notes)\n",
        "        print(f\"Input shape: {self.network_input.shape}\")\n",
        "        print(f\"Output shape: {self.network_output.shape}\")\n",
        "\n",
        "    def get_notes(self):\n",
        "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\n",
        "        # This is assuming that every interval between notes is the same (0.5)\n",
        "        notes = []\n",
        "\n",
        "        for file in glob.glob(\"midi_songs/*.mid\"):\n",
        "            midi = converter.parse(file)\n",
        "\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            notes_to_parse = None\n",
        "\n",
        "            try:  # file has instrument parts\n",
        "                s2 = instrument.partitionByInstrument(midi)\n",
        "                notes_to_parse = s2.parts[0].recurse()\n",
        "            except:  # file has notes in a flat structure\n",
        "                notes_to_parse = midi.flat.notes\n",
        "\n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
        "\n",
        "        with open(notes_dir, \"wb\") as filepath:\n",
        "            pickle.dump(notes, filepath)\n",
        "\n",
        "        return notes\n",
        "\n",
        "    def prepare_sequences(self, notes):\n",
        "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\n",
        "        self.n_vocab = len(set(notes))\n",
        "\n",
        "        # get all pitch names\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "        # create a dictionary to map pitches to integers\n",
        "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
        "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
        "\n",
        "        network_input = []\n",
        "        network_output = []\n",
        "\n",
        "        # create input sequences and the corresponding outputs\n",
        "        for i in range(0, len(self.notes) - self.seq_length, 1):\n",
        "            sequence_in = self.notes[i : i + self.seq_length]\n",
        "            sequence_out = self.notes[i + self.seq_length]\n",
        "            network_input.append([self.note_to_int[char] for char in sequence_in])\n",
        "            network_output.append(self.note_to_int[sequence_out])\n",
        "\n",
        "        n_patterns = len(network_input)\n",
        "\n",
        "        # reshape the input into a format compatible with LSTM layers\n",
        "        # So this is actuallyt (number of different inputs, sequence length, number of features)\n",
        "        network_input = np.reshape(network_input, (n_patterns, self.seq_length, 1))\n",
        "        # normalize input\n",
        "        network_input = network_input / float(self.n_vocab)\n",
        "\n",
        "        # network_output = to_categorical(network_output)\n",
        "\n",
        "        return (tensor(network_input, device=self.device), tensor(network_output, device=self.device))\n",
        "\n",
        "    def create_midi_from_model(self, prediction_output, filename):\n",
        "        \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "        offset = 0\n",
        "        output_notes = []\n",
        "\n",
        "        # create note and chord objects based on the values generated by the model\n",
        "        for pattern in prediction_output:\n",
        "            # pattern is a chord\n",
        "            if ('.' in pattern) or pattern.isdigit():\n",
        "                notes_in_chord = pattern.split('.')\n",
        "                notes = []\n",
        "                for current_note in notes_in_chord:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Piano()\n",
        "                    notes.append(new_note)\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "            # pattern is a note\n",
        "            else:\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.offset = offset\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                output_notes.append(new_note)\n",
        "\n",
        "            # increase offset each iteration so that notes do not stack\n",
        "            offset += 0.5\n",
        "\n",
        "        midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "        midi_stream.write('midi', fp=filename)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIMMZq2j2cPh"
      },
      "source": [
        "# QLSTM.py\n",
        "\n",
        "Embedding = Union[emb.AngleEmbedding, emb.AmplitudeEmbedding, emb.BasisEmbedding]\n",
        "Layer = Union[\n",
        "    lay.BasicEntanglerLayers,\n",
        "    lay.ParticleConservingU1,\n",
        "    lay.ParticleConservingU2,\n",
        "    lay.RandomLayers,\n",
        "    lay.StronglyEntanglingLayers,\n",
        "]\n",
        "\n",
        "\n",
        "class QLSTMCell(nn.Module):\n",
        "    def quantum_op(\n",
        "        self,\n",
        "        wires,\n",
        "        embedding: Embedding = emb.AngleEmbedding,\n",
        "        layer: Layer = lay.BasicEntanglerLayers,\n",
        "    ):\n",
        "        def circuit_part(inputs, weights):\n",
        "            embedding(inputs, wires=wires)\n",
        "            layer(weights, wires=wires)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in wires]\n",
        "\n",
        "        return circuit_part\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        n_qubits=4,\n",
        "        n_qlayers=1,\n",
        "        dropout=0,\n",
        "        batch_first=True,\n",
        "        return_sequences=False,\n",
        "        return_state=True,\n",
        "        backend=\"default.qubit\",\n",
        "        device=\"cpu\"\n",
        "    ):\n",
        "        super(QLSTMCell, self).__init__()\n",
        "        self.n_inputs = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.concat_size = self.n_inputs + self.hidden_size\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_qlayers = n_qlayers\n",
        "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
        "        self.device = device # \"cpu\", \"cuda\"\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.return_sequences = return_sequences\n",
        "        self.return_state = return_state\n",
        "\n",
        "        # self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
        "        # self.dev = qml.device('qiskit.basicaer', wires=self.n_qubits)\n",
        "        # self.dev = qml.device('qiskit.ibm', wires=self.n_qubits)\n",
        "        # use 'qiskit.ibmq' instead to run on hardware\n",
        "\n",
        "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
        "\n",
        "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
        "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
        "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
        "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
        "\n",
        "        self.qlayer_forget = qml.QNode(\n",
        "            self.quantum_op(self.wires_forget), self.dev_forget, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_input = qml.QNode(\n",
        "            self.quantum_op(self.wires_input), self.dev_input, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_update = qml.QNode(\n",
        "            self.quantum_op(self.wires_update), self.dev_update, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_output = qml.QNode(\n",
        "            self.quantum_op(self.wires_output), self.dev_output, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
        "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
        "\n",
        "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\n",
        "        self.VQC = {\n",
        "            \"forget\": qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes).to(device),\n",
        "            \"input\": qml.qnn.TorchLayer(self.qlayer_input, weight_shapes).to(device),\n",
        "            \"update\": qml.qnn.TorchLayer(self.qlayer_update, weight_shapes).to(device),\n",
        "            \"output\": qml.qnn.TorchLayer(self.qlayer_output, weight_shapes).to(device),\n",
        "        }\n",
        "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
        "        # self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        \"\"\"\n",
        "        x.shape is (batch_size, seq_length, feature_size)\n",
        "        recurrent_activation -> sigmoid\n",
        "        activation -> tanh\n",
        "        \"\"\"\n",
        "        # Automatically assumes single batch\n",
        "        x = x.to(self.device)\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.reshape(1, x.shape[0], x.shape[1])\n",
        "        \n",
        "        if self.batch_first is True:\n",
        "            batch_size, seq_length, features_size = x.size()\n",
        "        else:\n",
        "            seq_length, batch_size, features_size = x.size()\n",
        "\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t = torch.zeros(batch_size, self.hidden_size, device=self.device) # hidden state (output)\n",
        "            c_t = torch.zeros(batch_size, self.hidden_size, device=self.device) # cell state\n",
        "        else:\n",
        "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
        "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
        "            h_t, c_t = init_states\n",
        "            # h_t = h_t[0]\n",
        "            # c_t = c_t[0]\n",
        "\n",
        "        for t in range(seq_length):\n",
        "            # get features from the t-th element in seq, for all entries in the batch\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Concatenate input and hidden state\n",
        "            v_t = torch.cat((h_t, x_t), dim=1).float().to(device)\n",
        "\n",
        "            # match qubit dimension\n",
        "            y_t = self.clayer_in(v_t).to(self.device)\n",
        "            \n",
        "            f_t = torch.sigmoid(\n",
        "                self.clayer_out(self.VQC[\"forget\"](y_t).to(self.device))\n",
        "            ).to(self.device)  # forget block\n",
        "            i_t = torch.sigmoid(self.clayer_out(self.VQC[\"input\"](y_t).to(self.device)))  # input block\n",
        "            g_t = torch.tanh(self.clayer_out(self.VQC[\"update\"](y_t).to(self.device)))  # update block\n",
        "            o_t = torch.sigmoid(\n",
        "                self.clayer_out(self.VQC[\"output\"](y_t).to(self.device)).to(self.device)\n",
        "            ).to(self.device)  # output block\n",
        "\n",
        "            c_t = (f_t * c_t) + (i_t * g_t)\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        # Wow, such pseudo-keras!\n",
        "        h_t, c_t = h_t.float(), c_t.float()\n",
        "\n",
        "        if self.dropout:\n",
        "          F.dropout(h_t, self.dropout, inplace=True)\n",
        "\n",
        "        if self.return_state:\n",
        "            if self.return_sequences:\n",
        "                return hidden_seq, (h_t, c_t)\n",
        "            else:\n",
        "                return (h_t, c_t)\n",
        "        else:\n",
        "            if self.return_sequences:\n",
        "                return hidden_seq\n",
        "            else:\n",
        "                return h_t\n",
        "    \n",
        "    def predict(self, x, init_states=None):\n",
        "        return self.forward(x, init_states)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE5yYIB3XKu3"
      },
      "source": [
        "class QLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size: int, hidden_size: int, n_layers: int, n_qubits: list, n_qlayers: list, dropouts: list, backend=\"default.qubit\", device=\"cpu\"):\n",
        "    super(QLSTM, self).__init__()\n",
        "    self.models = nn.ModuleList()\n",
        "    self.n_layers = n_layers\n",
        "    for i in range(self.n_layers):\n",
        "      self.models.append(\n",
        "          QLSTMCell(input_size, hidden_size, n_qubits[i], n_qlayers[i], dropouts[i], backend=backend, device=device)\n",
        "      )\n",
        "    \n",
        "  def forward(self, note_sequences):\n",
        "    # A tuple of (h_t, c_t)\n",
        "    outputs = []\n",
        "    h_t_c_t = None\n",
        "    for i in range(self.n_layers):\n",
        "      h_t_c_t = self.models[i](note_sequences[i], h_t_c_t)\n",
        "      # Only output c_t\n",
        "      outputs.append(h_t_c_t[1])\n",
        "    \n",
        "    # print(outputs)\n",
        "    return torch.stack(outputs)\n",
        "\n",
        "  def predict(self, note_sequences):\n",
        "    return self.forward(note_sequences)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AgEdPUc2cPk"
      },
      "source": [
        "# LSTMusic.py\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch import optim\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# from QLTSM.qlstm import QLSTM\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LSTMusic(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_qlayers=1,\n",
        "        n_layers=1,\n",
        "        n_vocab=None,\n",
        "        input_dim=1,\n",
        "        hidden_dim=512,\n",
        "        n_qubits=4,\n",
        "        backend=\"default.qubit\",\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        super(LSTMusic, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        if n_qubits > 0:\n",
        "            print(f\"Generator will use Quantum LSTM running on backend {backend}\")\n",
        "            r_n = range(n_layers)\n",
        "            n_qubits = [4 for _ in r_n]\n",
        "            n_qlayers = [n_qlayers for _ in r_n]\n",
        "            dropouts = [0.3 for _ in r_n]\n",
        "            self.model = QLSTM(input_dim, hidden_dim, n_layers, n_qubits, n_qlayers, dropouts, device=device).to(device)\n",
        "\n",
        "            # self.model = QLSTM(\n",
        "            #     input_dim,\n",
        "            #     hidden_dim,\n",
        "            #     n_qubits=n_qubits,\n",
        "            #     backend=backend,\n",
        "            #     return_state=True,\n",
        "            #     device=device,\n",
        "            # ).to(device)\n",
        "        else:\n",
        "            print(\"Generator will use Classical LSTM\")\n",
        "            self.model = nn.LSTM(input_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "\n",
        "    def forward(self, note_sequences):\n",
        "        ct_list = self.model(note_sequences)\n",
        "        scores = []\n",
        "        for c_t in ct_list:\n",
        "          scores.append(F.log_softmax(c_t, dim=1))\n",
        "        # (h_t, c_t) = self.model(note_sequences)\n",
        "        # c_t = self.model(note_sequence)\n",
        "        # scores = F.log_softmax(c_t, dim=1)\n",
        "        ct_list = torch.stack(scores)\n",
        "        return ct_list.reshape(ct_list.shape[0], ct_list.shape[2])\n",
        "\n",
        "        # embeds = self.word_embeddings(sentence)\n",
        "        # lstm_out, _ = self.model(embeds.view(len(sentence), 1, -1))\n",
        "        # tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        # tag_scores = F.log_softmax(tag_logits, dim=1)\n",
        "        # return tag_scores\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        mode=True,\n",
        "        inputs=None,\n",
        "        outputs=None,\n",
        "        n_epochs=None,\n",
        "        cutoff: int = None,\n",
        "        learning_rate=0.1,\n",
        "    ):\n",
        "        # Same as categorical cross entropy, who would've thought?!\n",
        "        if mode == False:\n",
        "            return\n",
        "        loss_function = nn.NLLLoss()\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        if cutoff:\n",
        "            inputs = inputs[:cutoff]\n",
        "            outputs = outputs[:cutoff]\n",
        "\n",
        "        history = {\"loss\": []}\n",
        "\n",
        "        midi_data = list(zip(inputs, outputs))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            counter = 0\n",
        "            losses = []\n",
        "\n",
        "            for i in range(0, len(midi_data) - self.n_layers, self.n_layers):\n",
        "              data = midi_data[i:i+self.n_layers]\n",
        "              note_seqs = [datum[0] for datum in data]\n",
        "              next_notes = torch.stack([datum[1] for datum in data])\n",
        "              self.zero_grad()\n",
        "              c_t_list = self(note_seqs)\n",
        "              # c_t_list = c_t_list.reshape(c_t_list.shape[0], c_t_list.shape[2])\n",
        "              # print(c_t_list.shape, next_notes.reshape(self.n_layers).shape)\n",
        "              loss = loss_function(c_t_list, next_notes.reshape(self.n_layers).long())\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              losses.append(float(loss))\n",
        "              if counter % 5 == 0:\n",
        "                print(f\"On datapoint #{counter} out of {cutoff}\")\n",
        "              counter += 1\n",
        "\n",
        "            avg_loss = np.mean(losses)\n",
        "            history[\"loss\"].append(avg_loss)\n",
        "            print(\"Epoch {} / {}: Loss = {:.3f}\".format(epoch + 1, n_epochs, avg_loss))\n",
        "        return history\n",
        "\n",
        "    def generate_notes(self, network_input, int_to_note, n_vocab, n_notes):\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "        # pick a random sequence from the input as a starting point for the prediction\n",
        "        with torch.no_grad():\n",
        "            start = np.random.randint(0, len(network_input) - 1)\n",
        "\n",
        "            pattern = network_input[start]\n",
        "            prediction_output = []\n",
        "\n",
        "            # generate 500 notes\n",
        "            for _ in range(n_notes):\n",
        "                prediction_input = pattern.clone().detach().reshape(1, len(pattern), 1)\n",
        "                # prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "                (h_t, prediction) = self.model.predict(prediction_input)\n",
        "\n",
        "                index = prediction.argmax()\n",
        "                result = int_to_note[int(index)]\n",
        "                prediction_output.append(result)\n",
        "\n",
        "                added_index = (index / n_vocab).reshape(1, 1)\n",
        "\n",
        "                pattern = torch.cat((pattern, added_index), 0)\n",
        "                # pattern.append(index)\n",
        "                pattern = pattern[1 : len(pattern)]\n",
        "\n",
        "            return prediction_output\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0r6iXZ82cPm"
      },
      "source": [
        "seq_length = 100\n",
        "n_epochs = 1\n",
        "cutoff = 500\n",
        "n_qubits = 4\n",
        "n_layers = 4\n",
        "n_qlayers = 2\n",
        "\n",
        "model_name = f\"lstm{n_layers}-seq{seq_length}-cut{cutoff}-epcs{n_epochs}-qu{n_qubits}-nq{n_qlayers}\"\n",
        "model_str = f\"{model_name}.pt\""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a33Nq6eh2cPm",
        "outputId": "02775e16-ede3-4889-c280-19a1ae3b988b"
      },
      "source": [
        "print(\"Initialized Midi\")\n",
        "midi = Midi(seq_length, device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Midi\n",
            "Input shape: torch.Size([44806, 50, 1])\n",
            "Output shape: torch.Size([44806])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rkZC4Yb2cPn",
        "outputId": "5fcd32c7-280d-4ec4-f23e-56f759f97a03"
      },
      "source": [
        "print(\"Initialized LSTM\")\n",
        "lstm = LSTMusic(n_qlayers=n_qlayers, n_layers=n_layers, hidden_dim=midi.n_vocab, device=device).to(device)\n",
        "\n",
        "# TODO: Separate input data to test/train\n",
        "\n",
        "if Path(model_str).is_file():\n",
        "    print(\"Loading model\")\n",
        "    lstm.load_state_dict(torch.load(model_str))\n",
        "    lstm.eval()\n",
        "    # lstm = torch.load(model_str)\n",
        "else:\n",
        "    print(\"Training LSTM\")\n",
        "    train_history = lstm.train(\n",
        "        True, midi.network_input, midi.network_output, n_epochs=n_epochs, cutoff=cutoff\n",
        "    )\n",
        "    torch.save(lstm.state_dict(), model_str)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized LSTM\n",
            "Generator will use Quantum LSTM running on backend default.qubit\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 4)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 4)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 4)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 4)\n",
            "Training LSTM\n",
            "On datapoint #0 out of 500\n",
            "On datapoint #5 out of 500\n",
            "On datapoint #10 out of 500\n",
            "On datapoint #15 out of 500\n",
            "On datapoint #20 out of 500\n",
            "On datapoint #25 out of 500\n",
            "On datapoint #30 out of 500\n",
            "On datapoint #35 out of 500\n",
            "On datapoint #40 out of 500\n",
            "On datapoint #45 out of 500\n",
            "On datapoint #50 out of 500\n",
            "On datapoint #55 out of 500\n",
            "On datapoint #60 out of 500\n",
            "On datapoint #65 out of 500\n",
            "On datapoint #70 out of 500\n",
            "On datapoint #75 out of 500\n",
            "On datapoint #80 out of 500\n",
            "On datapoint #85 out of 500\n",
            "On datapoint #90 out of 500\n",
            "On datapoint #95 out of 500\n",
            "On datapoint #100 out of 500\n",
            "On datapoint #105 out of 500\n",
            "On datapoint #110 out of 500\n",
            "On datapoint #115 out of 500\n",
            "On datapoint #120 out of 500\n",
            "Epoch 1 / 1: Loss = 5.467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTxPm8XrDuLa"
      },
      "source": [
        "def generate_notes(self, network_input, int_to_note, n_vocab, n_notes, n_layers):\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "        # pick a random sequence from the input as a starting point for the prediction\n",
        "        with torch.no_grad():\n",
        "            req_size = n_notes//n_layers\n",
        "            start = np.random.randint(0, len(network_input) - req_size)\n",
        "\n",
        "            # pattern = network_input[start]\n",
        "            prediction_output = []\n",
        "\n",
        "            # generate n_notes\n",
        "            for i in range(start, start + n_notes, n_layers):\n",
        "                # print(network_input[i:i+n_layers].shape)\n",
        "                prediction_input = network_input[i:i+n_layers]\n",
        "                # prediction_input = pattern.clone().detach().reshape(1, len(pattern), 1)\n",
        "                # prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "                ct_list = self.model.predict(prediction_input)\n",
        "\n",
        "                for prediction in ct_list:\n",
        "                  index = prediction.argmax()\n",
        "                  result = int_to_note[int(index)]\n",
        "                  prediction_output.append(result)\n",
        "\n",
        "                # added_index = (index / n_vocab).reshape(1, 1)\n",
        "                # pattern = torch.cat((pattern, added_index), 0)\n",
        "                # pattern.append(index)\n",
        "                # pattern = pattern[1 : len(pattern)]\n",
        "\n",
        "            return prediction_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7PVNr3_2cPo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "b50baa23-1fdb-4764-fa74-d7c228c2c0df"
      },
      "source": [
        "print(\"Generating notes\")\n",
        "notes = generate_notes(\n",
        "    lstm, midi.network_input, midi.int_to_note, midi.n_vocab, n_notes=20, n_layers=3\n",
        ")\n",
        "notes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating notes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0d17b1727e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating notes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m notes = generate_notes(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_to_note\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_notes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mnotes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChgF6LoQ2cPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86f4e61-1be0-4892-c4c2-1e031512e262"
      },
      "source": [
        "print(\"Saving as MIDI file.\")\n",
        "midi.create_midi_from_model(notes, f\"{model_name}_generated.mid\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving as MIDI file.\n"
          ]
        }
      ]
    }
  ]
}