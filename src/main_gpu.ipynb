{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of main_gpu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "interpreter": {
      "hash": "d2c77ea8d338d5dbf262d683b98c2873f87d453475070b582f093a0c2749b897"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjdvyD62otP"
      },
      "source": [
        "!pip install git+https://github.com/PennyLaneAI/pennylane"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z70RfQb52cPX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.templates import embeddings as emb\n",
        "from pennylane.templates import layers as lay\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "import glob, pickle, time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y11nHDFJ79-C",
        "outputId": "144d94db-f3d0-4505-9815-b052b3717c5f"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "uKEIMOeK4VQV",
        "outputId": "d91c43b1-c9ca-4b94-eee2-2f218cde039a"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "uploaded.keys()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c910d87-0cbd-4841-8c11-fb9cc67a8f11\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c910d87-0cbd-4841-8c11-fb9cc67a8f11\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving notes.pk to notes.pk\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['notes.pk'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyTkc6uS2cPb"
      },
      "source": [
        "# Midi.py\n",
        "\n",
        "notes_dir = \"notes.pk\"\n",
        "\n",
        "class Midi:\n",
        "    def __init__(self, seq_length, device):\n",
        "        self.seq_length = seq_length\n",
        "        self.device = device\n",
        "\n",
        "        if Path(notes_dir).is_file():\n",
        "            self.notes = pickle.load(open(notes_dir, \"rb\"))\n",
        "            # self.notes = pickle.loads(uploaded[notes_dir])\n",
        "        else:\n",
        "            self.notes = self.get_notes()\n",
        "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\n",
        "\n",
        "        self.network_input, self.network_output = self.prepare_sequences(self.notes)\n",
        "        print(f\"Input shape: {self.network_input.shape}\")\n",
        "        print(f\"Output shape: {self.network_output.shape}\")\n",
        "\n",
        "    def get_notes(self):\n",
        "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\n",
        "        # This is assuming that every interval between notes is the same (0.5)\n",
        "        notes = []\n",
        "\n",
        "        for file in glob.glob(\"midi_songs/*.mid\"):\n",
        "            midi = converter.parse(file)\n",
        "\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            notes_to_parse = None\n",
        "\n",
        "            try:  # file has instrument parts\n",
        "                s2 = instrument.partitionByInstrument(midi)\n",
        "                notes_to_parse = s2.parts[0].recurse()\n",
        "            except:  # file has notes in a flat structure\n",
        "                notes_to_parse = midi.flat.notes\n",
        "\n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
        "\n",
        "        with open(notes_dir, \"wb\") as filepath:\n",
        "            pickle.dump(notes, filepath)\n",
        "\n",
        "        return notes\n",
        "\n",
        "    def prepare_sequences(self, notes):\n",
        "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\n",
        "        self.n_vocab = len(set(notes))\n",
        "\n",
        "        # get all pitch names\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "        # create a dictionary to map pitches to integers\n",
        "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
        "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
        "\n",
        "        network_input = []\n",
        "        network_output = []\n",
        "\n",
        "        # create input sequences and the corresponding outputs\n",
        "        for i in range(0, len(self.notes) - self.seq_length, 1):\n",
        "            sequence_in = self.notes[i : i + self.seq_length]\n",
        "            sequence_out = self.notes[i + self.seq_length]\n",
        "            network_input.append([self.note_to_int[char] for char in sequence_in])\n",
        "            network_output.append(self.note_to_int[sequence_out])\n",
        "\n",
        "        n_patterns = len(network_input)\n",
        "\n",
        "        # reshape the input into a format compatible with LSTM layers\n",
        "        # So this is actuallyt (number of different inputs, sequence length, number of features)\n",
        "        network_input = np.reshape(network_input, (n_patterns, self.seq_length, 1))\n",
        "        # normalize input\n",
        "        network_input = network_input / float(self.n_vocab)\n",
        "\n",
        "        # network_output = to_categorical(network_output)\n",
        "\n",
        "        return (tensor(network_input, device=self.device), tensor(network_output, device=self.device))\n",
        "\n",
        "    def create_midi_from_model(self, prediction_output, filename):\n",
        "        \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "        offset = 0\n",
        "        output_notes = []\n",
        "\n",
        "        # create note and chord objects based on the values generated by the model\n",
        "        for pattern in prediction_output:\n",
        "            # pattern is a chord\n",
        "            if ('.' in pattern) or pattern.isdigit():\n",
        "                notes_in_chord = pattern.split('.')\n",
        "                notes = []\n",
        "                for current_note in notes_in_chord:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Piano()\n",
        "                    notes.append(new_note)\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "            # pattern is a note\n",
        "            else:\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.offset = offset\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                output_notes.append(new_note)\n",
        "\n",
        "            # increase offset each iteration so that notes do not stack\n",
        "            offset += 0.5\n",
        "\n",
        "        midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "        midi_stream.write('midi', fp=filename)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIMMZq2j2cPh"
      },
      "source": [
        "# QLSTM.py\n",
        "\n",
        "Embedding = Union[emb.AngleEmbedding, emb.AmplitudeEmbedding, emb.BasisEmbedding]\n",
        "Layer = Union[\n",
        "    lay.BasicEntanglerLayers,\n",
        "    lay.ParticleConservingU1,\n",
        "    lay.ParticleConservingU2,\n",
        "    lay.RandomLayers,\n",
        "    lay.StronglyEntanglingLayers,\n",
        "]\n",
        "\n",
        "\n",
        "class QLSTMCell(nn.Module):\n",
        "    def quantum_op(\n",
        "        self,\n",
        "        wires,\n",
        "        embedding: Embedding = emb.AngleEmbedding,\n",
        "        layer: Layer = lay.BasicEntanglerLayers,\n",
        "    ):\n",
        "        def circuit_part(inputs, weights):\n",
        "            embedding(inputs, wires=wires)\n",
        "            layer(weights, wires=wires)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in wires]\n",
        "\n",
        "        return circuit_part\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        n_qubits=4,\n",
        "        n_qlayers=1,\n",
        "        dropout=0,\n",
        "        batch_first=True,\n",
        "        return_sequences=False,\n",
        "        return_state=True,\n",
        "        backend=\"default.qubit\",\n",
        "        device=\"cpu\"\n",
        "    ):\n",
        "        super(QLSTMCell, self).__init__()\n",
        "        self.n_inputs = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.concat_size = self.n_inputs + self.hidden_size\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_qlayers = n_qlayers\n",
        "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
        "        self.device = device # \"cpu\", \"cuda\"\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.return_sequences = return_sequences\n",
        "        self.return_state = return_state\n",
        "\n",
        "        # self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
        "        # self.dev = qml.device('qiskit.basicaer', wires=self.n_qubits)\n",
        "        # self.dev = qml.device('qiskit.ibm', wires=self.n_qubits)\n",
        "        # use 'qiskit.ibmq' instead to run on hardware\n",
        "\n",
        "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
        "\n",
        "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
        "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
        "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
        "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
        "\n",
        "        self.qlayer_forget = qml.QNode(\n",
        "            self.quantum_op(self.wires_forget), self.dev_forget, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_input = qml.QNode(\n",
        "            self.quantum_op(self.wires_input), self.dev_input, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_update = qml.QNode(\n",
        "            self.quantum_op(self.wires_update), self.dev_update, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_output = qml.QNode(\n",
        "            self.quantum_op(self.wires_output), self.dev_output, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
        "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
        "\n",
        "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\n",
        "        self.VQC = {\n",
        "            \"forget\": qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes).to(device),\n",
        "            \"input\": qml.qnn.TorchLayer(self.qlayer_input, weight_shapes).to(device),\n",
        "            \"update\": qml.qnn.TorchLayer(self.qlayer_update, weight_shapes).to(device),\n",
        "            \"output\": qml.qnn.TorchLayer(self.qlayer_output, weight_shapes).to(device),\n",
        "        }\n",
        "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
        "        # self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        \"\"\"\n",
        "        x.shape is (batch_size, seq_length, feature_size)\n",
        "        recurrent_activation -> sigmoid\n",
        "        activation -> tanh\n",
        "        \"\"\"\n",
        "        # Automatically assumes single batch\n",
        "        x = x.to(self.device)\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.reshape(1, x.shape[0], x.shape[1])\n",
        "        \n",
        "        if self.batch_first is True:\n",
        "            batch_size, seq_length, features_size = x.size()\n",
        "        else:\n",
        "            seq_length, batch_size, features_size = x.size()\n",
        "\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t = torch.zeros(batch_size, self.hidden_size, device=self.device) # hidden state (output)\n",
        "            c_t = torch.zeros(batch_size, self.hidden_size, device=self.device) # cell state\n",
        "        else:\n",
        "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
        "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
        "            h_t, c_t = init_states\n",
        "            # h_t = h_t[0]\n",
        "            # c_t = c_t[0]\n",
        "\n",
        "        for t in range(seq_length):\n",
        "            # get features from the t-th element in seq, for all entries in the batch\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Concatenate input and hidden state\n",
        "            v_t = torch.cat((h_t, x_t), dim=1).float().to(device)\n",
        "\n",
        "            # match qubit dimension\n",
        "            y_t = self.clayer_in(v_t).to(self.device)\n",
        "            \n",
        "            f_t = torch.sigmoid(\n",
        "                self.clayer_out(self.VQC[\"forget\"](y_t).to(self.device))\n",
        "            ).to(self.device)  # forget block\n",
        "            i_t = torch.sigmoid(self.clayer_out(self.VQC[\"input\"](y_t).to(self.device)))  # input block\n",
        "            g_t = torch.tanh(self.clayer_out(self.VQC[\"update\"](y_t).to(self.device)))  # update block\n",
        "            o_t = torch.sigmoid(\n",
        "                self.clayer_out(self.VQC[\"output\"](y_t).to(self.device)).to(self.device)\n",
        "            ).to(self.device)  # output block\n",
        "\n",
        "            c_t = (f_t * c_t) + (i_t * g_t)\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        # Wow, such pseudo-keras!\n",
        "        h_t, c_t = h_t.float(), c_t.float()\n",
        "\n",
        "        if self.dropout:\n",
        "          F.dropout(h_t, self.dropout, inplace=True)\n",
        "\n",
        "        if self.return_state:\n",
        "            if self.return_sequences:\n",
        "                return hidden_seq, (h_t, c_t)\n",
        "            else:\n",
        "                return (h_t, c_t)\n",
        "        else:\n",
        "            if self.return_sequences:\n",
        "                return hidden_seq\n",
        "            else:\n",
        "                return h_t\n",
        "    \n",
        "    def predict(self, x, init_states=None):\n",
        "        return self.forward(x, init_states)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE5yYIB3XKu3"
      },
      "source": [
        "class QLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size: int, hidden_size: int, n_layers: int, n_qubits: list, n_qlayers: list, dropouts: list, backend=\"default.qubit\", device=\"cpu\"):\n",
        "    super(QLSTM, self).__init__()\n",
        "    self.models = nn.ModuleList()\n",
        "    self.n_layers = n_layers\n",
        "    for i in range(self.n_layers):\n",
        "      self.models.append(\n",
        "          QLSTMCell(input_size, hidden_size, n_qubits[i], n_qlayers[i], dropouts[i], backend=backend, device=device)\n",
        "      )\n",
        "    \n",
        "  def forward(self, note_sequences):\n",
        "    # A tuple of (h_t, c_t)\n",
        "    outputs = []\n",
        "    h_t_c_t = None\n",
        "    for i in range(self.n_layers):\n",
        "      h_t_c_t = self.models[i](note_sequences[i], h_t_c_t)\n",
        "      # Only output c_t\n",
        "      outputs.append(h_t_c_t[1])\n",
        "    \n",
        "    # print(outputs)\n",
        "    return torch.stack(outputs)\n",
        "\n",
        "  def predict(self, note_sequences):\n",
        "    return self.forward(note_sequences)\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AgEdPUc2cPk"
      },
      "source": [
        "# LSTMusic.py\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch import optim\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# from QLTSM.qlstm import QLSTM\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LSTMusic(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers=1,\n",
        "        n_vocab=None,\n",
        "        input_dim=1,\n",
        "        hidden_dim=512,\n",
        "        n_qubits=4,\n",
        "        backend=\"default.qubit\",\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        super(LSTMusic, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        if n_qubits > 0:\n",
        "            print(f\"Generator will use Quantum LSTM running on backend {backend}\")\n",
        "            r_n = range(n_layers)\n",
        "            n_qubits = [4 for _ in r_n]\n",
        "            n_qlayers = [1 for _ in r_n]\n",
        "            dropouts = [0.3 for _ in r_n]\n",
        "            self.model = QLSTM(input_dim, hidden_dim, n_layers, n_qubits, n_qlayers, dropouts, device=device).to(device)\n",
        "\n",
        "            # self.model = QLSTM(\n",
        "            #     input_dim,\n",
        "            #     hidden_dim,\n",
        "            #     n_qubits=n_qubits,\n",
        "            #     backend=backend,\n",
        "            #     return_state=True,\n",
        "            #     device=device,\n",
        "            # ).to(device)\n",
        "        else:\n",
        "            print(\"Generator will use Classical LSTM\")\n",
        "            self.model = nn.LSTM(input_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "\n",
        "    def forward(self, note_sequences):\n",
        "        ct_list = self.model(note_sequences)\n",
        "        scores = []\n",
        "        for c_t in ct_list:\n",
        "          scores.append(F.log_softmax(c_t, dim=1))\n",
        "        # (h_t, c_t) = self.model(note_sequences)\n",
        "        # c_t = self.model(note_sequence)\n",
        "        # scores = F.log_softmax(c_t, dim=1)\n",
        "        return torch.stack(scores)\n",
        "\n",
        "        # embeds = self.word_embeddings(sentence)\n",
        "        # lstm_out, _ = self.model(embeds.view(len(sentence), 1, -1))\n",
        "        # tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        # tag_scores = F.log_softmax(tag_logits, dim=1)\n",
        "        # return tag_scores\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        mode=True,\n",
        "        inputs=None,\n",
        "        outputs=None,\n",
        "        n_epochs=None,\n",
        "        cutoff: int = None,\n",
        "        learning_rate=0.1,\n",
        "    ):\n",
        "        # Same as categorical cross entropy, who would've thought?!\n",
        "        if mode == False:\n",
        "            return\n",
        "        loss_function = nn.NLLLoss()\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        if cutoff:\n",
        "            inputs = inputs[:cutoff]\n",
        "            outputs = outputs[:cutoff]\n",
        "\n",
        "        history = {\"loss\": []}\n",
        "\n",
        "        midi_data = list(zip(inputs, outputs))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            counter = 0\n",
        "            losses = []\n",
        "\n",
        "            for i in range(0, len(midi_data) - self.n_layers, self.n_layers):\n",
        "              data = midi_data[i:i+self.n_layers]\n",
        "              note_seqs = [datum[0] for datum in data]\n",
        "              next_notes = torch.stack([datum[1] for datum in data])\n",
        "              self.zero_grad()\n",
        "              c_t_list = self(note_seqs)\n",
        "              loss = loss_function(c_t_list, next_notes.reshape(self.n_layers).long())\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              losses.append(float(loss))\n",
        "              if counter % 5 == 0:\n",
        "                print(f\"On datapoint #{counter} out of {cutoff}\")\n",
        "              counter += 1\n",
        "\n",
        "            # for note_series, next_note in zip(inputs, outputs):\n",
        "            #     # Step 1. Remember that Pytorch accumulates gradients.\n",
        "            #     # We need to clear them out before each instance\n",
        "            #     self.zero_grad()\n",
        "\n",
        "            #     # Step 2. Run our forward pass.\n",
        "            #     c_t = self(note_series)\n",
        "            #     if epoch == 1:\n",
        "            #       print(c_t)\n",
        "            #     # print(c_t.shape, next_note.reshape(1).shape)\n",
        "\n",
        "            #     # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "            #     #  calling optimizer.step()\n",
        "            #     loss = loss_function(c_t, next_note.reshape(1).long())\n",
        "            #     loss.backward()\n",
        "            #     optimizer.step()\n",
        "            #     losses.append(float(loss))\n",
        "            #     if counter % 5 == 0:\n",
        "            #         print(f\"On datapoint #{counter} out of {cutoff}\")\n",
        "            #     counter += 1\n",
        "\n",
        "            avg_loss = np.mean(losses)\n",
        "            history[\"loss\"].append(avg_loss)\n",
        "            print(\"Epoch {} / {}: Loss = {:.3f}\".format(epoch + 1, n_epochs, avg_loss))\n",
        "        return history\n",
        "\n",
        "    def generate_notes(self, network_input, int_to_note, n_vocab, n_notes):\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "        # pick a random sequence from the input as a starting point for the prediction\n",
        "        with torch.no_grad():\n",
        "            start = np.random.randint(0, len(network_input) - 1)\n",
        "\n",
        "            pattern = network_input[start]\n",
        "            prediction_output = []\n",
        "\n",
        "            # generate 500 notes\n",
        "            for _ in range(n_notes):\n",
        "                prediction_input = pattern.clone().detach().reshape(1, len(pattern), 1)\n",
        "                # prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "                (h_t, prediction) = self.model.predict(prediction_input)\n",
        "\n",
        "                index = prediction.argmax()\n",
        "                result = int_to_note[int(index)]\n",
        "                prediction_output.append(result)\n",
        "\n",
        "                added_index = (index / n_vocab).reshape(1, 1)\n",
        "\n",
        "                pattern = torch.cat((pattern, added_index), 0)\n",
        "                # pattern.append(index)\n",
        "                pattern = pattern[1 : len(pattern)]\n",
        "\n",
        "            return prediction_output\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0r6iXZ82cPm"
      },
      "source": [
        "seq_length = 50\n",
        "n_epochs = 2\n",
        "cutoff = 100\n",
        "n_qubits = 4\n",
        "\n",
        "model_name = f\"lstm-seq{seq_length}-cut{cutoff}-epcs{n_epochs}-qu{n_qubits}\"\n",
        "model_str = f\"{model_name}.pt\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a33Nq6eh2cPm",
        "outputId": "673ac06a-d72b-48e2-938b-1ef951e02123"
      },
      "source": [
        "print(\"Initialized Midi\")\n",
        "midi = Midi(seq_length, device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Midi\n",
            "Input shape: torch.Size([44806, 50, 1])\n",
            "Output shape: torch.Size([44806])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "-rkZC4Yb2cPn",
        "outputId": "886a59a7-d79f-47c9-c17b-1c06d44b5a58"
      },
      "source": [
        "print(\"Initialized LSTM\")\n",
        "lstm = LSTMusic(n_layers=3, hidden_dim=midi.n_vocab, device=device).to(device)\n",
        "\n",
        "if Path(model_str).is_file():\n",
        "    print(\"Loading model\")\n",
        "    lstm.load_state_dict(torch.load(model_str))\n",
        "    lstm.eval()\n",
        "    # lstm = torch.load(model_str)\n",
        "else:\n",
        "    print(\"Training LSTM\")\n",
        "    train_history = lstm.train(\n",
        "        True, midi.network_input, midi.network_output, n_epochs=n_epochs, cutoff=cutoff\n",
        "    )\n",
        "    torch.save(lstm.state_dict(), model_str)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized LSTM\n",
            "Generator will use Quantum LSTM running on backend default.qubit\n",
            "weight_shapes = (n_qlayers, n_qubits) = (1, 4)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (1, 4)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (1, 4)\n",
            "Training LSTM\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-830286852746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training LSTM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     train_history = lstm.train(\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-e9f3e29e10a0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode, inputs, outputs, n_epochs, cutoff, learning_rate)\u001b[0m\n\u001b[1;32m     99\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m               \u001b[0mc_t_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_t_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_notes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [3, 308], got [3]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTxPm8XrDuLa"
      },
      "source": [
        "def generate_notes(self, network_input, int_to_note, n_vocab, n_notes):\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "        # pick a random sequence from the input as a starting point for the prediction\n",
        "        with torch.no_grad():\n",
        "            start = np.random.randint(0, len(network_input) - 1)\n",
        "\n",
        "            pattern = network_input[start]\n",
        "            prediction_output = []\n",
        "\n",
        "            # generate 500 notes\n",
        "            for _ in range(n_notes):\n",
        "                prediction_input = pattern.clone().detach().reshape(1, len(pattern), 1)\n",
        "                # prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "                (h_t, prediction) = self.model.predict(prediction_input)\n",
        "\n",
        "                index = prediction.argmax()\n",
        "                result = int_to_note[int(index)]\n",
        "                prediction_output.append(result)\n",
        "\n",
        "                added_index = (index / n_vocab).reshape(1, 1)\n",
        "\n",
        "                pattern = torch.cat((pattern, added_index), 0)\n",
        "                # pattern.append(index)\n",
        "                pattern = pattern[1 : len(pattern)]\n",
        "\n",
        "            return prediction_output"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7PVNr3_2cPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cd9722-e355-4694-d606-76edb013da41"
      },
      "source": [
        "print(\"Generating notes\")\n",
        "notes = generate_notes(\n",
        "    lstm, midi.network_input, midi.int_to_note, midi.n_vocab, n_notes=50\n",
        ")\n",
        "notes"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating notes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9',\n",
              " '5.9']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChgF6LoQ2cPq",
        "outputId": "c159e104-57b1-4754-d1ec-bae75cce03ba"
      },
      "source": [
        "print(\"Saving as MIDI file.\")\n",
        "midi.create_midi_from_model(notes, f\"{model_name}_generated.mid\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving as MIDI file.\n"
          ]
        }
      ]
    }
  ]
}