{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of QuGan-pennyLane-improved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "dc44aa18a01d4ef3c49dce97499f9883b1b573f96a33e2e79347f993e3562639"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9NOkxg_pBM_"
      },
      "source": [
        "!pip install git+https://github.com/PennyLaneAI/pennylane"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAb_ay37ZFxl"
      },
      "source": [
        "import pennylane as qml\n",
        "\n",
        "from pennylane.templates.layers import BasicEntanglerLayers, StronglyEntanglingLayers, RandomLayers\n",
        "from pennylane.templates.embeddings import AmplitudeEmbedding\n",
        "import pennylane.numpy as np\n",
        "import torch\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "\n",
        "from pathlib import Path\n",
        "import pickle, glob "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJArgegTZtVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f33007f-81aa-43e9-9cc8-3014837593bb"
      },
      "source": [
        "n_wires = 10\n",
        "wires_range = range(n_wires)\n",
        "\n",
        "n_note_encoding = 6 \n",
        "encoding_range = range(n_note_encoding)\n",
        "\n",
        "dev = qml.device('default.qubit', wires=n_wires)\n",
        "\n",
        "running_dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "running_dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BztExfRotBp"
      },
      "source": [
        "!wget https://github.com/theerfan/Maqenta/raw/main/data/notes.pk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYUGm4mUotBs"
      },
      "source": [
        "# Midi.py\n",
        "\n",
        "notes_dir = \"notes.pk\"\n",
        "\n",
        "\n",
        "class Midi:\n",
        "    def __init__(self, seq_length, device):\n",
        "        self.seq_length = seq_length\n",
        "        self.device = device\n",
        "\n",
        "        if Path(notes_dir).is_file():\n",
        "            self.notes = pickle.load(open(notes_dir, \"rb\"))\n",
        "            # self.notes = pickle.loads(uploaded[notes_dir])\n",
        "        else:\n",
        "            self.notes = self.get_notes()\n",
        "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\n",
        "\n",
        "        self.network_input, self.network_output = self.prepare_sequences(self.notes)\n",
        "        print(f\"Input shape: {self.network_input.shape}\")\n",
        "        print(f\"Output shape: {self.network_output.shape}\")\n",
        "\n",
        "    def get_notes(self):\n",
        "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\n",
        "        # This is assuming that every interval between notes is the same (0.5)\n",
        "        notes = []\n",
        "\n",
        "        for file in glob.glob(\"midi_songs/*.mid\"):\n",
        "            midi = converter.parse(file)\n",
        "\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            notes_to_parse = None\n",
        "\n",
        "            try:  # file has instrument parts\n",
        "                s2 = instrument.partitionByInstrument(midi)\n",
        "                notes_to_parse = s2.parts[0].recurse()\n",
        "            except:  # file has notes in a flat structure\n",
        "                notes_to_parse = midi.flat.notes\n",
        "\n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
        "\n",
        "        with open(notes_dir, \"wb\") as filepath:\n",
        "            pickle.dump(notes, filepath)\n",
        "\n",
        "        return notes\n",
        "\n",
        "    def prepare_sequences(self, notes):\n",
        "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\n",
        "        self.n_vocab = len(set(notes))\n",
        "\n",
        "        # get all pitch names\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "        # create a dictionary to map pitches to integers\n",
        "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
        "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
        "\n",
        "        network_input = []\n",
        "        network_output = []\n",
        "\n",
        "        # create input sequences and the corresponding outputs\n",
        "        for i in range(len(self.notes) - self.seq_length):\n",
        "            sequence_in = self.notes[i : i + self.seq_length]\n",
        "            sequence_out = self.notes[i + self.seq_length]\n",
        "            network_input.append([self.note_to_int[char] for char in sequence_in])\n",
        "            network_output.append(self.note_to_int[sequence_out])\n",
        "\n",
        "        n_patterns = len(network_input)\n",
        "\n",
        "        # reshape the input into a format compatible with LSTM layers\n",
        "        # So this is actuallyt (number of different inputs, sequence length, number of features)\n",
        "        network_input = np.reshape(network_input, (n_patterns, self.seq_length))\n",
        "        network_input = torch.tensor(network_input, device=self.device, dtype=torch.double)\n",
        "\n",
        "        self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=1))\n",
        "        \n",
        "        # print(network_input.shape)\n",
        "        for i in range(network_input.shape[0]):\n",
        "            network_input[i] /= self.input_norms[i]\n",
        "        # network_input = torch.div(network_input, self.input_norms)\n",
        "\n",
        "        return (\n",
        "            network_input,\n",
        "            torch.tensor(network_output, device=self.device),\n",
        "        )\n",
        "\n",
        "    def create_midi_from_model(self, prediction_output, filename):\n",
        "        \"\"\"convert the output from the prediction to notes and create a midi file\n",
        "        from the notes\"\"\"\n",
        "        offset = 0\n",
        "        output_notes = []\n",
        "\n",
        "        # create note and chord objects based on the values generated by the model\n",
        "        for pattern in prediction_output:\n",
        "            # pattern is a chord\n",
        "            if (\".\" in pattern) or pattern.isdigit():\n",
        "                notes_in_chord = pattern.split(\".\")\n",
        "                notes = []\n",
        "                for current_note in notes_in_chord:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Piano()\n",
        "                    notes.append(new_note)\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "            # pattern is a note\n",
        "            else:\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.offset = offset\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                output_notes.append(new_note)\n",
        "\n",
        "            # increase offset each iteration so that notes do not stack\n",
        "            offset += 0.5\n",
        "\n",
        "        midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "        midi_stream.write(\"midi\", fp=filename)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzWrWeDotBw",
        "outputId": "3f5069cd-1ecb-42d4-84ed-66090879f6a9"
      },
      "source": [
        "seq_length = 2 **  n_note_encoding\n",
        "print(\"Initialized Midi\")\n",
        "midi = Midi(seq_length, running_dev)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Midi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([44792, 64])\n",
            "Output shape: torch.Size([44792])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdxqkcOUotB2"
      },
      "source": [
        "def real_music(notes):\n",
        "    AmplitudeEmbedding(features=notes, wires=encoding_range, normalize=True)\n",
        "\n",
        "def music_generator(weights):\n",
        "    # StronglyEntanglingLayers(weights, wires=encoding_range)\n",
        "    # BasicEntanglerLayers(weights, wires=encoding_range)\n",
        "    RandomLayers(weights, wires=encoding_range)\n",
        "\n",
        "def discriminator(weights):\n",
        "    # BasicEntanglerLayers(weights, wires=wires_range)\n",
        "    StronglyEntanglingLayers(weights, wires=wires_range)\n",
        "\n",
        "def measurement(wire_count):\n",
        "    obs = qml.PauliZ(0)\n",
        "    for i in range(1, wire_count):\n",
        "        obs = obs @ qml.PauliZ(i)\n",
        "    return qml.expval(obs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA3oT7fAotB4"
      },
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def real_music_discriminator(inputs, weights):\n",
        "    real_music(inputs)\n",
        "    discriminator(weights)\n",
        "    return measurement(n_note_encoding)\n",
        "\n",
        "def music_generator_circuit(inputs, note_weights):\n",
        "  real_music(inputs)\n",
        "  music_generator(note_weights)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def generated_music_discriminator(inputs, note_weights, weights):\n",
        "    music_generator_circuit(inputs, note_weights)\n",
        "    discriminator(weights)\n",
        "    return measurement(n_note_encoding)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFDG_HgotB6"
      },
      "source": [
        "n_disc_layers = 8\n",
        "n_gen_layers = 12\n",
        "\n",
        "real_shapes = {\"weights\": (n_disc_layers, n_wires, 3)}\n",
        "\n",
        "real_layer = qml.qnn.TorchLayer(real_music_discriminator, real_shapes).to(running_dev)\n",
        "\n",
        "generated_shapes = {\n",
        "    \"weights\": (n_disc_layers, n_wires, 3),\n",
        "    \"note_weights\": (n_gen_layers, n_note_encoding),\n",
        "}\n",
        "\n",
        "generated_layer = qml.qnn.TorchLayer(generated_music_discriminator, generated_shapes).to(running_dev)\n",
        "generated_layer.weights.requires_grad=False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G_cIU7HotB8"
      },
      "source": [
        "def sync_weights(source_layer, target_layer):\n",
        "    \"\"\"Synchronize the weights of two layers\"\"\"\n",
        "    source_weights = source_layer.weights\n",
        "    target_weights = target_layer.weights\n",
        "    with torch.no_grad():\n",
        "        for source_weight, target_weight in zip(source_weights, target_weights):\n",
        "            target_weight.data = source_weight.data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHOALwB2otB9"
      },
      "source": [
        "def prob_fun_disc_true(layer):\n",
        "    def prob_true(inputs):\n",
        "        true_output = layer(inputs)\n",
        "        # Convert to probability\n",
        "        prob_true = (true_output + 1) / 2\n",
        "        return prob_true\n",
        "\n",
        "    return prob_true"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e_tQb8zotB-"
      },
      "source": [
        "prob_real_true = prob_fun_disc_true(real_layer)\n",
        "prob_gen_true = prob_fun_disc_true(generated_layer)\n",
        "\n",
        "empty_input = torch.tensor(np.zeros((1,))).to(running_dev)\n",
        "\n",
        "def disc_cost(inputs):\n",
        "    return prob_gen_true(inputs) - prob_real_true(inputs)\n",
        "\n",
        "def gen_cost(inputs):\n",
        "    return -prob_gen_true(inputs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBNO6Q3totB_"
      },
      "source": [
        "def gen_batch_inputs(batch_size=1):\n",
        "    return midi.network_input[\n",
        "        np.random.randint(0, len(midi.network_input), size=batch_size)\n",
        "    ]\n",
        "\n",
        "def shuffle_music(datapoint):\n",
        "  return datapoint[torch.randperm(datapoint.size()[0])].detach()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4LqIAqsotCA"
      },
      "source": [
        "def discriminator_iteration(n_iterations, batch_size, learning_rate):\n",
        "\n",
        "    opt = torch.optim.Adam(real_layer.parameters(), lr=learning_rate)\n",
        "    best_cost = disc_cost(midi.network_input[0])\n",
        "    \n",
        "    for _ in range(n_iterations):\n",
        "        opt.zero_grad()\n",
        "        # Sample a batch of data\n",
        "        batch_inputs = gen_batch_inputs()\n",
        "        # batch_inputs = gen_batch_inputs(batch_size)\n",
        "        # batch_inputs = batch_inputs / midi.input_norms[:batch_size]\n",
        "        batch_inputs = batch_inputs.detach()\n",
        "        # Compute the loss\n",
        "        loss = disc_cost(batch_inputs)\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        opt.step()\n",
        "        # Update the best cost\n",
        "        if loss < best_cost:\n",
        "            best_cost = loss\n",
        "    print(\"New best Discriminator cost:\", best_cost)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VHdC5YOotCC"
      },
      "source": [
        "def generator_iteration(n_iterations, learning_rate):\n",
        "    opt = torch.optim.SGD(filter(lambda p: p.requires_grad, generated_layer.parameters()), lr=learning_rate)\n",
        "    best_cost = gen_cost(midi.network_input[0])\n",
        "    \n",
        "    for _ in range(n_iterations):\n",
        "        opt.zero_grad()\n",
        "        # Compute the loss\n",
        "\n",
        "        batch_inputs = gen_batch_inputs()\n",
        "        batch_inputs = shuffle_music(batch_inputs)\n",
        "\n",
        "        # print(generated_layer.note_weights)\n",
        "\n",
        "        loss = gen_cost(batch_inputs)\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        opt.step()\n",
        "        # Update the best cost\n",
        "        if loss < best_cost:\n",
        "            best_cost = loss\n",
        "    print(\"New best Generator cost:\", best_cost)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a6dvXrjotCC"
      },
      "source": [
        "# The real iteration\n",
        "steps = 100\n",
        "n_iterations = 20\n",
        "learning_rate = 0.1\n",
        "batch_size = 3\n",
        "\n",
        "for _ in range(steps):\n",
        "    discriminator_iteration(n_iterations, batch_size, learning_rate)\n",
        "    sync_weights(real_layer, generated_layer)\n",
        "    generator_iteration(n_iterations, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BWz5DsXfmjj",
        "outputId": "eaa8c40a-d6b0-4243-d051-d1e15dab5caa"
      },
      "source": [
        "torch.max(midi.input_norms)\n",
        "len(midi.int_to_note)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL0mp3QF7_ca"
      },
      "source": [
        "def generate_notes(model, network_input, int_to_note, n_notes):\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "        # pick a random sequence from the input as a starting point for the prediction\n",
        "        scale_factor = len(midi.int_to_note) / torch.max(midi.input_norms)\n",
        "        with torch.no_grad():\n",
        "            start = np.random.randint(0, len(network_input) - n_notes)\n",
        "\n",
        "            # pattern = network_input[start]\n",
        "            prediction_output = []\n",
        "\n",
        "            # generate n_notes\n",
        "            for i in range(start, start + n_notes):\n",
        "                input_ = network_input[i]\n",
        "                generated_note = model(shuffle_music(input_)) \n",
        "                generated_note = (generated_note + 1)/2 * scale_factor * midi.input_norms[i]\n",
        "                result = int_to_note[int(generated_note)]\n",
        "                prediction_output.append(result)\n",
        "\n",
        "            return prediction_output"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEqA6ewB-qh7"
      },
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def final_music_generator(inputs, note_weights):\n",
        "  music_generator_circuit(inputs, note_weights)\n",
        "  return measurement(n_note_encoding)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhnWOuio8JBk"
      },
      "source": [
        "# generator_only = qml.QNode(final_music_generator, dev, interface=\"torch\")\n",
        "weight_gens = {\n",
        "    \"note_weights\": (n_gen_layers, n_note_encoding),\n",
        "}\n",
        "generator_only_layer = qml.qnn.TorchLayer(final_music_generator, weight_gens).to(running_dev)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKPGkZ6ATXHo"
      },
      "source": [
        "def sync_final_weights(source_layer, target_layer):\n",
        "    \"\"\"Synchronize the weights of two layers\"\"\"\n",
        "    source_weights = source_layer.note_weights\n",
        "    target_weights = target_layer.note_weights\n",
        "    with torch.no_grad():\n",
        "        for source_weight, target_weight in zip(source_weights, target_weights):\n",
        "            target_weight.data = source_weight.data"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucnsxtJS3T__",
        "outputId": "e536f932-2190-40db-f45a-55a164c94ad0"
      },
      "source": [
        "n_notes = 100\n",
        "generated_notes = []\n",
        "print(\"Generating notes\")\n",
        "sync_final_weights(generated_layer, generator_only_layer)\n",
        "notes = generate_notes(generator_only_layer, midi.network_input, midi.int_to_note, n_notes=n_notes)\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating notes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edu8KMc3-JKk"
      },
      "source": [
        "model_name = f\"quGan-1\"\n",
        "model_str = f\"{model_name}.pt\""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rABcRRUG-LiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1136bb-89fa-42ef-a963-d2ba868661be"
      },
      "source": [
        "print(\"Saving as MIDI file.\")\n",
        "midi.create_midi_from_model(notes, f\"{model_name}_generated.mid\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving as MIDI file.\n"
          ]
        }
      ]
    }
  ]
}