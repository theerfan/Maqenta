{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of QuGan-pennyLane-improved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "dc44aa18a01d4ef3c49dce97499f9883b1b573f96a33e2e79347f993e3562639"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9NOkxg_pBM_",
        "outputId": "0a7cffd5-1968-4c8d-b982-6e877cfd27a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+https://github.com/PennyLaneAI/pennylane"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PennyLaneAI/pennylane\n",
            "  Cloning https://github.com/PennyLaneAI/pennylane to /tmp/pip-req-build-h9nryfbh\n",
            "  Running command git clone -q https://github.com/PennyLaneAI/pennylane /tmp/pip-req-build-h9nryfbh\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (2.6.3)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.4.4)\n",
            "Collecting semantic_version==2.6\n",
            "  Downloading semantic_version-2.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting autoray\n",
            "  Downloading autoray-0.2.5-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (4.2.2)\n",
            "Collecting pennylane-lightning>=0.18\n",
            "  Downloading PennyLane_Lightning-0.18.0-cp37-cp37m-manylinux2010_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd->PennyLane==0.19.0.dev0) (0.16.0)\n",
            "Building wheels for collected packages: PennyLane\n",
            "  Building wheel for PennyLane (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PennyLane: filename=PennyLane-0.19.0.dev0-py3-none-any.whl size=666683 sha256=63006a375bba7597b793f11a323ba8e55b558f0333c4135dfd5c7aac126843a9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l2b6inwe/wheels/9e/4b/fe/27dcf8ba174161f9d1af1841251dc97013a33a66f16cd8d661\n",
            "Successfully built PennyLane\n",
            "Installing collected packages: semantic-version, pennylane-lightning, autoray, PennyLane\n",
            "Successfully installed PennyLane-0.19.0.dev0 autoray-0.2.5 pennylane-lightning-0.18.0 semantic-version-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAb_ay37ZFxl"
      },
      "source": [
        "import pennylane as qml\n",
        "\n",
        "from pennylane.templates.layers import BasicEntanglerLayers, StronglyEntanglingLayers, RandomLayers\n",
        "from pennylane.templates.embeddings import AmplitudeEmbedding\n",
        "import pennylane.numpy as np\n",
        "import torch\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "\n",
        "from pathlib import Path\n",
        "import pickle, glob "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJArgegTZtVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b31b83-d1fc-4473-f1f5-e6f735c7d113"
      },
      "source": [
        "n_wires = 10\n",
        "wires_range = range(n_wires)\n",
        "\n",
        "n_note_encoding = 6 \n",
        "encoding_range = range(n_note_encoding)\n",
        "\n",
        "dev = qml.device('default.qubit', wires=n_wires)\n",
        "\n",
        "running_dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "running_dev"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BztExfRotBp",
        "outputId": "2a7b3f6d-5f61-4059-d797-65937363cb97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://github.com/theerfan/Maqenta/raw/main/data/notes.pk"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-04 17:19:08--  https://github.com/theerfan/Maqenta/raw/main/data/notes.pk\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/theerfan/Maqenta/main/data/notes.pk [following]\n",
            "--2021-10-04 17:19:08--  https://raw.githubusercontent.com/theerfan/Maqenta/main/data/notes.pk\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 250124 (244K) [application/octet-stream]\n",
            "Saving to: ‘notes.pk’\n",
            "\n",
            "notes.pk            100%[===================>] 244.26K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-10-04 17:19:08 (8.10 MB/s) - ‘notes.pk’ saved [250124/250124]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYUGm4mUotBs"
      },
      "source": [
        "# Midi.py\n",
        "\n",
        "notes_dir = \"notes.pk\"\n",
        "\n",
        "\n",
        "class Midi:\n",
        "    def __init__(self, seq_length, device):\n",
        "        self.seq_length = seq_length\n",
        "        self.device = device\n",
        "\n",
        "        if Path(notes_dir).is_file():\n",
        "            self.notes = pickle.load(open(notes_dir, \"rb\"))\n",
        "            # self.notes = pickle.loads(uploaded[notes_dir])\n",
        "        else:\n",
        "            self.notes = self.get_notes()\n",
        "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\n",
        "\n",
        "        self.network_input, self.network_output = self.prepare_sequences(self.notes)\n",
        "        print(f\"Input shape: {self.network_input.shape}\")\n",
        "        print(f\"Output shape: {self.network_output.shape}\")\n",
        "\n",
        "    def get_notes(self):\n",
        "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\n",
        "        # This is assuming that every interval between notes is the same (0.5)\n",
        "        notes = []\n",
        "\n",
        "        for file in glob.glob(\"midi_songs/*.mid\"):\n",
        "            midi = converter.parse(file)\n",
        "\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            notes_to_parse = None\n",
        "\n",
        "            try:  # file has instrument parts\n",
        "                s2 = instrument.partitionByInstrument(midi)\n",
        "                notes_to_parse = s2.parts[0].recurse()\n",
        "            except:  # file has notes in a flat structure\n",
        "                notes_to_parse = midi.flat.notes\n",
        "\n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
        "\n",
        "        with open(notes_dir, \"wb\") as filepath:\n",
        "            pickle.dump(notes, filepath)\n",
        "\n",
        "        return notes\n",
        "\n",
        "    def prepare_sequences(self, notes):\n",
        "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\n",
        "        self.n_vocab = len(set(notes))\n",
        "\n",
        "        # get all pitch names\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "        # create a dictionary to map pitches to integers\n",
        "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
        "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
        "\n",
        "        network_input = []\n",
        "        network_output = []\n",
        "\n",
        "        # create input sequences and the corresponding outputs\n",
        "        for i in range(len(self.notes) - self.seq_length):\n",
        "            sequence_in = self.notes[i : i + self.seq_length]\n",
        "            sequence_out = self.notes[i + self.seq_length]\n",
        "            network_input.append([self.note_to_int[char] for char in sequence_in])\n",
        "            network_output.append(self.note_to_int[sequence_out])\n",
        "\n",
        "        n_patterns = len(network_input)\n",
        "\n",
        "        # reshape the input into a format compatible with LSTM layers\n",
        "        # So this is actuallyt (number of different inputs, sequence length, number of features)\n",
        "        network_input = np.reshape(network_input, (n_patterns, self.seq_length))\n",
        "        network_input = torch.tensor(network_input, device=self.device, dtype=torch.double)\n",
        "\n",
        "        self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=1))\n",
        "        \n",
        "        # print(network_input.shape)\n",
        "        for i in range(network_input.shape[0]):\n",
        "            network_input[i] /= self.input_norms[i]\n",
        "        # network_input = torch.div(network_input, self.input_norms)\n",
        "\n",
        "        return (\n",
        "            network_input,\n",
        "            torch.tensor(network_output, device=self.device),\n",
        "        )\n",
        "\n",
        "    def create_midi_from_model(self, prediction_output, filename):\n",
        "        \"\"\"convert the output from the prediction to notes and create a midi file\n",
        "        from the notes\"\"\"\n",
        "        offset = 0\n",
        "        output_notes = []\n",
        "\n",
        "        # create note and chord objects based on the values generated by the model\n",
        "        for pattern in prediction_output:\n",
        "            # pattern is a chord\n",
        "            if (\".\" in pattern) or pattern.isdigit():\n",
        "                notes_in_chord = pattern.split(\".\")\n",
        "                notes = []\n",
        "                for current_note in notes_in_chord:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Piano()\n",
        "                    notes.append(new_note)\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "            # pattern is a note\n",
        "            else:\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.offset = offset\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                output_notes.append(new_note)\n",
        "\n",
        "            # increase offset each iteration so that notes do not stack\n",
        "            offset += 0.5\n",
        "\n",
        "        midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "        midi_stream.write(\"midi\", fp=filename)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzWrWeDotBw",
        "outputId": "9803f763-9da9-4196-f2f4-6b93d87a7e48"
      },
      "source": [
        "seq_length = 2 **  n_note_encoding\n",
        "print(\"Initialized Midi\")\n",
        "midi = Midi(seq_length, running_dev)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Midi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([44792, 64])\n",
            "Output shape: torch.Size([44792])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdxqkcOUotB2"
      },
      "source": [
        "def real_music(notes):\n",
        "    AmplitudeEmbedding(features=notes, wires=encoding_range, normalize=True)\n",
        "\n",
        "def music_generator(weights):\n",
        "    # StronglyEntanglingLayers(weights, wires=encoding_range)\n",
        "    # BasicEntanglerLayers(weights, wires=encoding_range)\n",
        "    RandomLayers(weights, wires=encoding_range)\n",
        "\n",
        "def discriminator(weights):\n",
        "    # BasicEntanglerLayers(weights, wires=wires_range)\n",
        "    StronglyEntanglingLayers(weights, wires=wires_range)\n",
        "\n",
        "def measurement(wire_count):\n",
        "    obs = qml.PauliZ(0)\n",
        "    for i in range(1, wire_count):\n",
        "        obs = obs @ qml.PauliZ(i)\n",
        "    return qml.expval(obs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA3oT7fAotB4"
      },
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def real_music_discriminator(inputs, weights):\n",
        "    real_music(inputs)\n",
        "    discriminator(weights)\n",
        "    return measurement(n_note_encoding)\n",
        "\n",
        "def music_generator_circuit(inputs, note_weights):\n",
        "  real_music(inputs)\n",
        "  music_generator(note_weights)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def generated_music_discriminator(inputs, note_weights, weights):\n",
        "    music_generator_circuit(inputs, note_weights)\n",
        "    discriminator(weights)\n",
        "    return measurement(n_note_encoding)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFDG_HgotB6"
      },
      "source": [
        "n_disc_layers = 8\n",
        "n_gen_layers = 12\n",
        "\n",
        "real_shapes = {\"weights\": (n_disc_layers, n_wires, 3)}\n",
        "\n",
        "real_layer = qml.qnn.TorchLayer(real_music_discriminator, real_shapes).to(running_dev)\n",
        "\n",
        "generated_shapes = {\n",
        "    \"weights\": (n_disc_layers, n_wires, 3),\n",
        "    \"note_weights\": (n_gen_layers, n_note_encoding),\n",
        "}\n",
        "\n",
        "generated_layer = qml.qnn.TorchLayer(generated_music_discriminator, generated_shapes).to(running_dev)\n",
        "generated_layer.weights.requires_grad=False"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G_cIU7HotB8"
      },
      "source": [
        "def sync_weights(source_layer, target_layer):\n",
        "    \"\"\"Synchronize the weights of two layers\"\"\"\n",
        "    source_weights = source_layer.weights\n",
        "    target_weights = target_layer.weights\n",
        "    with torch.no_grad():\n",
        "        for source_weight, target_weight in zip(source_weights, target_weights):\n",
        "            target_weight.data = source_weight.data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHOALwB2otB9"
      },
      "source": [
        "def prob_fun_disc_true(layer):\n",
        "    def prob_true(inputs):\n",
        "        true_output = layer(inputs)\n",
        "        # Convert to probability\n",
        "        prob_true = (true_output + 1) / 2\n",
        "        return prob_true\n",
        "\n",
        "    return prob_true"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e_tQb8zotB-"
      },
      "source": [
        "prob_real_true = prob_fun_disc_true(real_layer)\n",
        "prob_gen_true = prob_fun_disc_true(generated_layer)\n",
        "\n",
        "empty_input = torch.tensor(np.zeros((1,))).to(running_dev)\n",
        "\n",
        "def disc_cost(inputs):\n",
        "    return prob_gen_true(inputs) - prob_real_true(inputs)\n",
        "\n",
        "def gen_cost(inputs):\n",
        "    return -prob_gen_true(inputs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBNO6Q3totB_"
      },
      "source": [
        "def gen_batch_inputs(batch_size=1):\n",
        "    return midi.network_input[\n",
        "        np.random.randint(0, len(midi.network_input), size=batch_size)\n",
        "    ]\n",
        "\n",
        "def shuffle_music(datapoint):\n",
        "  return datapoint[torch.randperm(datapoint.size()[0])].detach()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4LqIAqsotCA"
      },
      "source": [
        "def discriminator_iteration(n_iterations, batch_size, learning_rate):\n",
        "\n",
        "    opt = torch.optim.Adam(real_layer.parameters(), lr=learning_rate)\n",
        "    best_cost = disc_cost(midi.network_input[0])\n",
        "    \n",
        "    for _ in range(n_iterations):\n",
        "        opt.zero_grad()\n",
        "        # Sample a batch of data\n",
        "        batch_inputs = gen_batch_inputs()\n",
        "        # batch_inputs = gen_batch_inputs(batch_size)\n",
        "        # batch_inputs = batch_inputs / midi.input_norms[:batch_size]\n",
        "        batch_inputs = batch_inputs.detach()\n",
        "        # Compute the loss\n",
        "        loss = disc_cost(batch_inputs)\n",
        "        sync_weights(real_layer, generated_layer)\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        opt.step()\n",
        "        # Update the best cost\n",
        "        if loss < best_cost:\n",
        "            best_cost = loss\n",
        "    print(\"New best Discriminator cost:\", best_cost)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VHdC5YOotCC"
      },
      "source": [
        "def generator_iteration(n_iterations, learning_rate):\n",
        "    opt = torch.optim.SGD(filter(lambda p: p.requires_grad, generated_layer.parameters()), lr=learning_rate)\n",
        "    best_cost = gen_cost(midi.network_input[0])\n",
        "    \n",
        "    for _ in range(n_iterations):\n",
        "        opt.zero_grad()\n",
        "        # Compute the loss\n",
        "\n",
        "        batch_inputs = gen_batch_inputs()\n",
        "        batch_inputs = shuffle_music(batch_inputs)\n",
        "\n",
        "        # print(generated_layer.note_weights)\n",
        "\n",
        "        loss = gen_cost(batch_inputs)\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        opt.step()\n",
        "        # Update the best cost\n",
        "        if loss < best_cost:\n",
        "            best_cost = loss\n",
        "    print(\"New best Generator cost:\", best_cost)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a6dvXrjotCC",
        "outputId": "d4fb4cd6-970b-4dd0-9ad2-0b062ba4a34c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The real iteration\n",
        "steps = 100\n",
        "n_iterations = 20\n",
        "learning_rate = 0.1\n",
        "batch_size = 3\n",
        "\n",
        "for _ in range(steps):\n",
        "    discriminator_iteration(n_iterations, batch_size, learning_rate)\n",
        "    # sync_weights(real_layer, generated_layer)\n",
        "    generator_iteration(n_iterations, learning_rate)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:149: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /pytorch/aten/src/ATen/native/Copy.cpp:240.)\n",
            "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best Discriminator cost: tensor(-0.2899, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5046, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.3121, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5234, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.3022, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5379, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2936, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5511, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2869, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5615, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2798, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5700, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2775, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5773, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2749, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5791, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2715, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5816, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2698, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5837, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2679, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5873, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2660, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5907, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2642, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5944, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2612, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5969, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2599, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.5996, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2572, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6024, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2561, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6046, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2524, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6072, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2493, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6110, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2480, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6120, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2434, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6171, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2427, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6201, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2395, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6224, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2376, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6239, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2363, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6261, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2338, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6272, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2315, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6293, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2313, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6311, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2282, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6321, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2280, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6346, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2268, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6349, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2252, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6361, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2221, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6366, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2221, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6364, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2216, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6391, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2218, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6386, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2217, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6385, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2203, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6400, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2210, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6404, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2196, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6408, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2190, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6411, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2185, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6414, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2172, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6417, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2165, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6423, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2163, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6440, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2173, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6438, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2167, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6425, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2154, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6450, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2161, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6453, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2157, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6445, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2150, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6456, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2153, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6450, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2146, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6446, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2157, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6457, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2137, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6471, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2138, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6467, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2128, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6466, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2137, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6477, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2120, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6494, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2133, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6476, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2145, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6474, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2115, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6478, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2119, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6467, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2116, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6479, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2117, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6481, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2107, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6507, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2109, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6498, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2122, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6492, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2111, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6487, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2094, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6478, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2098, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6496, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2098, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6490, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2104, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6496, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2109, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6499, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2105, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6498, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2103, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6500, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2119, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6500, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2091, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6500, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2087, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6506, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2095, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6505, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2083, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6515, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2091, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6501, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2103, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6500, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2089, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6503, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2093, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6518, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2096, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6532, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2093, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6512, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2086, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6514, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2091, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6527, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2099, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6509, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2064, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6502, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2082, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6526, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2075, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6517, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2059, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6506, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2057, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6511, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2079, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6519, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2083, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6518, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2076, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6505, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2079, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6517, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "New best Discriminator cost: tensor(-0.2072, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "New best Generator cost: tensor(-0.6512, dtype=torch.float64, grad_fn=<NegBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BWz5DsXfmjj",
        "outputId": "eaa8c40a-d6b0-4243-d051-d1e15dab5caa"
      },
      "source": [
        "torch.max(midi.input_norms)\n",
        "len(midi.int_to_note)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL0mp3QF7_ca"
      },
      "source": [
        "def generate_notes(model, network_input, int_to_note, n_notes):\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "        # pick a random sequence from the input as a starting point for the prediction\n",
        "        scale_factor = len(midi.int_to_note) / torch.max(midi.input_norms)\n",
        "        with torch.no_grad():\n",
        "            start = np.random.randint(0, len(network_input) - n_notes)\n",
        "\n",
        "            # pattern = network_input[start]\n",
        "            prediction_output = []\n",
        "\n",
        "            # generate n_notes\n",
        "            for i in range(start, start + n_notes):\n",
        "                input_ = network_input[i]\n",
        "                generated_note = model(shuffle_music(input_)) \n",
        "                generated_note = (generated_note + 1) * midi.input_norms[i]\n",
        "                generate_notes = int(generated_note)\n",
        "                counter = 1\n",
        "                while generated_note not in int_to_note:\n",
        "                    generated_note *= counter / (counter + 1)\n",
        "                    generated_note = int(generated_note)\n",
        "                    counter += 1\n",
        "                result = int_to_note[int(generated_note)]\n",
        "                prediction_output.append(result)\n",
        "\n",
        "            return prediction_output"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEqA6ewB-qh7"
      },
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def final_music_generator(inputs, note_weights):\n",
        "  music_generator_circuit(inputs, note_weights)\n",
        "  return measurement(n_note_encoding)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhnWOuio8JBk"
      },
      "source": [
        "# generator_only = qml.QNode(final_music_generator, dev, interface=\"torch\")\n",
        "weight_gens = {\n",
        "    \"note_weights\": (n_gen_layers, n_note_encoding),\n",
        "}\n",
        "generator_only_layer = qml.qnn.TorchLayer(final_music_generator, weight_gens).to(running_dev)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKPGkZ6ATXHo"
      },
      "source": [
        "def sync_final_weights(source_layer, target_layer):\n",
        "    \"\"\"Synchronize the weights of two layers\"\"\"\n",
        "    source_weights = source_layer.note_weights\n",
        "    target_weights = target_layer.note_weights\n",
        "    with torch.no_grad():\n",
        "        for source_weight, target_weight in zip(source_weights, target_weights):\n",
        "            target_weight.data = source_weight.data"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucnsxtJS3T__",
        "outputId": "2d27c9b2-c293-4f7c-f6a7-08eb6165119a"
      },
      "source": [
        "n_notes = 100\n",
        "generated_notes = []\n",
        "print(\"Generating notes\")\n",
        "sync_final_weights(generated_layer, generator_only_layer)\n",
        "notes = generate_notes(generator_only_layer, midi.network_input, midi.int_to_note, n_notes=n_notes)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating notes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edu8KMc3-JKk"
      },
      "source": [
        "model_name = f\"quGan-6\"\n",
        "model_str = f\"{model_name}.pt\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rABcRRUG-LiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719761fe-08ef-4427-b4cd-9d8106bb166e"
      },
      "source": [
        "print(\"Saving as MIDI file.\")\n",
        "midi.create_midi_from_model(notes, f\"{model_name}_generated.mid\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving as MIDI file.\n"
          ]
        }
      ]
    }
  ]
}