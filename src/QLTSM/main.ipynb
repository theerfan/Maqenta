{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch import Tensor\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "import pennylane as qml\r\n",
    "from pennylane import numpy as np\r\n",
    "from pennylane.templates import embeddings as emb\r\n",
    "from pennylane.templates import layers as lay\r\n",
    "\r\n",
    "from typing import Union\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from music21 import converter, instrument, note, chord, stream\r\n",
    "import glob, pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# midi.py\r\n",
    "notes_dir = \"data/notes.pk\"\r\n",
    "\r\n",
    "class Midi:\r\n",
    "    def __init__(self, seq_length):\r\n",
    "        self.seq_length = seq_length\r\n",
    "\r\n",
    "        if Path(notes_dir).is_file():\r\n",
    "            self.notes = pickle.load(open(notes_dir, \"rb\"))\r\n",
    "        else:\r\n",
    "            self.notes = self.get_notes()\r\n",
    "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\r\n",
    "\r\n",
    "        self.network_input, self.network_output = self.prepare_sequences(self.notes)\r\n",
    "        print(f\"Input shape: {self.network_input.shape}\")\r\n",
    "        print(f\"Output shape: {self.network_output.shape}\")\r\n",
    "\r\n",
    "    def get_notes(self):\r\n",
    "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\r\n",
    "        # This is assuming that every interval between notes is the same (0.5)\r\n",
    "        notes = []\r\n",
    "\r\n",
    "        for file in glob.glob(\"midi_songs/*.mid\"):\r\n",
    "            midi = converter.parse(file)\r\n",
    "\r\n",
    "            print(\"Parsing %s\" % file)\r\n",
    "\r\n",
    "            notes_to_parse = None\r\n",
    "\r\n",
    "            try:  # file has instrument parts\r\n",
    "                s2 = instrument.partitionByInstrument(midi)\r\n",
    "                notes_to_parse = s2.parts[0].recurse()\r\n",
    "            except:  # file has notes in a flat structure\r\n",
    "                notes_to_parse = midi.flat.notes\r\n",
    "\r\n",
    "            for element in notes_to_parse:\r\n",
    "                if isinstance(element, note.Note):\r\n",
    "                    notes.append(str(element.pitch))\r\n",
    "                elif isinstance(element, chord.Chord):\r\n",
    "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\r\n",
    "\r\n",
    "        with open(notes_dir, \"wb\") as filepath:\r\n",
    "            pickle.dump(notes, filepath)\r\n",
    "\r\n",
    "        return notes\r\n",
    "\r\n",
    "    def prepare_sequences(self, notes):\r\n",
    "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\r\n",
    "        self.n_vocab = len(set(notes))\r\n",
    "\r\n",
    "        # get all pitch names\r\n",
    "        pitchnames = sorted(set(item for item in notes))\r\n",
    "\r\n",
    "        # create a dictionary to map pitches to integers\r\n",
    "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\r\n",
    "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\r\n",
    "\r\n",
    "        network_input = []\r\n",
    "        network_output = []\r\n",
    "\r\n",
    "        # create input sequences and the corresponding outputs\r\n",
    "        for i in range(0, len(self.notes) - self.seq_length, 1):\r\n",
    "            sequence_in = self.notes[i : i + self.seq_length]\r\n",
    "            sequence_out = self.notes[i + self.seq_length]\r\n",
    "            network_input.append([self.note_to_int[char] for char in sequence_in])\r\n",
    "            network_output.append(self.note_to_int[sequence_out])\r\n",
    "\r\n",
    "        n_patterns = len(network_input)\r\n",
    "\r\n",
    "        # reshape the input into a format compatible with LSTM layers\r\n",
    "        # So this is actuallyt (number of different inputs, sequence length, number of features)\r\n",
    "        network_input = np.reshape(network_input, (n_patterns, self.seq_length, 1))\r\n",
    "        # normalize input\r\n",
    "        network_input = network_input / float(self.n_vocab)\r\n",
    "\r\n",
    "        # network_output = to_categorical(network_output)\r\n",
    "\r\n",
    "        return (Tensor(network_input), Tensor(network_output))\r\n",
    "\r\n",
    "    def create_midi_from_model(self, prediction_output, filename):\r\n",
    "        \"\"\" convert the output from the prediction to notes and create a midi file\r\n",
    "        from the notes \"\"\"\r\n",
    "        offset = 0\r\n",
    "        output_notes = []\r\n",
    "\r\n",
    "        # create note and chord objects based on the values generated by the model\r\n",
    "        for pattern in prediction_output:\r\n",
    "            # pattern is a chord\r\n",
    "            if ('.' in pattern) or pattern.isdigit():\r\n",
    "                notes_in_chord = pattern.split('.')\r\n",
    "                notes = []\r\n",
    "                for current_note in notes_in_chord:\r\n",
    "                    new_note = note.Note(int(current_note))\r\n",
    "                    new_note.storedInstrument = instrument.Piano()\r\n",
    "                    notes.append(new_note)\r\n",
    "                new_chord = chord.Chord(notes)\r\n",
    "                new_chord.offset = offset\r\n",
    "                output_notes.append(new_chord)\r\n",
    "            # pattern is a note\r\n",
    "            else:\r\n",
    "                new_note = note.Note(pattern)\r\n",
    "                new_note.offset = offset\r\n",
    "                new_note.storedInstrument = instrument.Piano()\r\n",
    "                output_notes.append(new_note)\r\n",
    "\r\n",
    "            # increase offset each iteration so that notes do not stack\r\n",
    "            offset += 0.5\r\n",
    "\r\n",
    "        midi_stream = stream.Stream(output_notes)\r\n",
    "\r\n",
    "        midi_stream.write('midi', fp=filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# QLSTM.py\r\n",
    "Embedding = Union[emb.AngleEmbedding, emb.AmplitudeEmbedding, emb.BasisEmbedding]\r\n",
    "Layer = Union[\r\n",
    "    lay.BasicEntanglerLayers,\r\n",
    "    lay.ParticleConservingU1,\r\n",
    "    lay.ParticleConservingU2,\r\n",
    "    lay.RandomLayers,\r\n",
    "    lay.StronglyEntanglingLayers,\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "class QLSTM(nn.Module):\r\n",
    "    def quantum_op(\r\n",
    "        self,\r\n",
    "        wires,\r\n",
    "        embedding: Embedding = emb.AngleEmbedding,\r\n",
    "        layer: Layer = lay.BasicEntanglerLayers,\r\n",
    "    ):\r\n",
    "        def circuit_part(inputs, weights):\r\n",
    "            embedding(inputs, wires=wires)\r\n",
    "            layer(weights, wires=wires)\r\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in wires]\r\n",
    "\r\n",
    "        return circuit_part\r\n",
    "\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        input_size,\r\n",
    "        hidden_size,\r\n",
    "        n_qubits=4,\r\n",
    "        n_qlayers=1,\r\n",
    "        batch_first=True,\r\n",
    "        return_sequences=False,\r\n",
    "        return_state=False,\r\n",
    "        backend=\"default.qubit\",\r\n",
    "    ):\r\n",
    "        super(QLSTM, self).__init__()\r\n",
    "        self.n_inputs = input_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\r\n",
    "        self.n_qubits = n_qubits\r\n",
    "        self.n_qlayers = n_qlayers\r\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\r\n",
    "\r\n",
    "        self.batch_first = batch_first\r\n",
    "        self.return_sequences = return_sequences\r\n",
    "        self.return_state = return_state\r\n",
    "\r\n",
    "        # self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\r\n",
    "        # self.dev = qml.device('qiskit.basicaer', wires=self.n_qubits)\r\n",
    "        # self.dev = qml.device('qiskit.ibm', wires=self.n_qubits)\r\n",
    "        # use 'qiskit.ibmq' instead to run on hardware\r\n",
    "\r\n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\r\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\r\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\r\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\r\n",
    "\r\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\r\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\r\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\r\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\r\n",
    "\r\n",
    "        self.qlayer_forget = qml.QNode(\r\n",
    "            self.quantum_op(self.wires_forget), self.dev_forget, interface=\"torch\"\r\n",
    "        )\r\n",
    "\r\n",
    "        self.qlayer_input = qml.QNode(\r\n",
    "            self.quantum_op(self.wires_input), self.dev_input, interface=\"torch\"\r\n",
    "        )\r\n",
    "\r\n",
    "        self.qlayer_update = qml.QNode(\r\n",
    "            self.quantum_op(self.wires_update), self.dev_update, interface=\"torch\"\r\n",
    "        )\r\n",
    "\r\n",
    "        self.qlayer_output = qml.QNode(\r\n",
    "            self.quantum_op(self.wires_output), self.dev_output, interface=\"torch\"\r\n",
    "        )\r\n",
    "\r\n",
    "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\r\n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\r\n",
    "\r\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\r\n",
    "        self.VQC = {\r\n",
    "            \"forget\": qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\r\n",
    "            \"input\": qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\r\n",
    "            \"update\": qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\r\n",
    "            \"output\": qml.qnn.TorchLayer(self.qlayer_output, weight_shapes),\r\n",
    "        }\r\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\r\n",
    "        # self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\r\n",
    "\r\n",
    "    def forward(self, x, init_states=None):\r\n",
    "        \"\"\"\r\n",
    "        x.shape is (batch_size, seq_length, feature_size)\r\n",
    "        recurrent_activation -> sigmoid\r\n",
    "        activation -> tanh\r\n",
    "        \"\"\"\r\n",
    "        # Automatically assumes single batch\r\n",
    "        if len(x.shape) == 2:\r\n",
    "            x = x.reshape(1, x.shape[0], x.shape[1])\r\n",
    "        \r\n",
    "        if self.batch_first is True:\r\n",
    "            batch_size, seq_length, features_size = x.size()\r\n",
    "        else:\r\n",
    "            seq_length, batch_size, features_size = x.size()\r\n",
    "\r\n",
    "        hidden_seq = []\r\n",
    "        if init_states is None:\r\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\r\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\r\n",
    "        else:\r\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\r\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\r\n",
    "            h_t, c_t = init_states\r\n",
    "            h_t = h_t[0]\r\n",
    "            c_t = c_t[0]\r\n",
    "\r\n",
    "        for t in range(seq_length):\r\n",
    "            # get features from the t-th element in seq, for all entries in the batch\r\n",
    "            x_t = x[:, t, :]\r\n",
    "\r\n",
    "            # Concatenate input and hidden state\r\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\r\n",
    "\r\n",
    "            # match qubit dimension\r\n",
    "            y_t = self.clayer_in(v_t)\r\n",
    "\r\n",
    "            f_t = torch.sigmoid(\r\n",
    "                self.clayer_out(self.VQC[\"forget\"](y_t))\r\n",
    "            )  # forget block\r\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC[\"input\"](y_t)))  # input block\r\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC[\"update\"](y_t)))  # update block\r\n",
    "            o_t = torch.sigmoid(\r\n",
    "                self.clayer_out(self.VQC[\"output\"](y_t))\r\n",
    "            )  # output block\r\n",
    "\r\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\r\n",
    "            h_t = o_t * torch.tanh(c_t)\r\n",
    "\r\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\r\n",
    "\r\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\r\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\r\n",
    "\r\n",
    "        # Wow, such pseudo-keras!\r\n",
    "        if self.return_state:\r\n",
    "            if self.return_sequences:\r\n",
    "                return hidden_seq, (h_t, c_t)\r\n",
    "            else:\r\n",
    "                return (h_t, c_t)\r\n",
    "        else:\r\n",
    "            if self.return_sequences:\r\n",
    "                return hidden_seq\r\n",
    "            else:\r\n",
    "                return h_t\r\n",
    "    \r\n",
    "    def predict(self, x, init_states=None):\r\n",
    "        return self.forward(x, init_states)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LSTMusic.py\r\n",
    "\r\n",
    "class LSTMusic(nn.Module):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        n_vocab=None,\r\n",
    "        input_dim=1,\r\n",
    "        hidden_dim=512,\r\n",
    "        n_qubits=4,\r\n",
    "        backend=\"default.qubit\",\r\n",
    "    ):\r\n",
    "        super(LSTMusic, self).__init__()\r\n",
    "        self.hidden_dim = hidden_dim\r\n",
    "\r\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n",
    "        # with dimensionality hidden_dim.\r\n",
    "        if n_qubits > 0:\r\n",
    "            print(f\"Generator will use Quantum LSTM running on backend {backend}\")\r\n",
    "\r\n",
    "            # self.model = nn.Sequential(\r\n",
    "            #     QLSTM(\r\n",
    "            #         input_size=input_dim,\r\n",
    "            #         hidden_size=hidden_dim,\r\n",
    "            #         n_qubits=n_qubits,\r\n",
    "            #         backend=backend,\r\n",
    "            #     ),\r\n",
    "            #     nn.Dropout(0.3),\r\n",
    "            #     QLSTM(\r\n",
    "            #         input_size=input_dim,\r\n",
    "            #         hidden_size=hidden_dim,\r\n",
    "            #         n_qubits=n_qubits,\r\n",
    "            #         backend=backend,\r\n",
    "            #     ),\r\n",
    "            #     nn.Dropout(0.3),\r\n",
    "            #     QLSTM(\r\n",
    "            #         input_size=input_dim,\r\n",
    "            #         hidden_size=hidden_dim,\r\n",
    "            #         n_qubits=n_qubits,\r\n",
    "            #         backend=backend,\r\n",
    "            #     ),\r\n",
    "            #     nn.Linear(hidden_dim, hidden_dim),\r\n",
    "            #     nn.Dropout(0.3),\r\n",
    "            #     nn.Linear(hidden_dim, n_vocab),\r\n",
    "            # )\r\n",
    "\r\n",
    "            self.model = QLSTM(\r\n",
    "                input_dim,\r\n",
    "                hidden_dim,\r\n",
    "                n_qubits=n_qubits,\r\n",
    "                backend=backend,\r\n",
    "                return_state=True,\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            print(\"Generator will use Classical LSTM\")\r\n",
    "            self.model = nn.LSTM(input_dim, hidden_dim)\r\n",
    "\r\n",
    "        # The linear layer that maps from hidden state space to tag space\r\n",
    "\r\n",
    "    def forward(self, note_sequence):\r\n",
    "        (h_t, c_t) = self.model(note_sequence)\r\n",
    "        scores = F.log_softmax(c_t, dim=1)\r\n",
    "        return scores\r\n",
    "\r\n",
    "        # embeds = self.word_embeddings(sentence)\r\n",
    "        # lstm_out, _ = self.model(embeds.view(len(sentence), 1, -1))\r\n",
    "        # tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\r\n",
    "        # tag_scores = F.log_softmax(tag_logits, dim=1)\r\n",
    "        # return tag_scores\r\n",
    "\r\n",
    "    def train(\r\n",
    "        self,\r\n",
    "        mode=True,\r\n",
    "        inputs=None,\r\n",
    "        outputs=None,\r\n",
    "        n_epochs=None,\r\n",
    "        cutoff: int = None,\r\n",
    "        learning_rate=0.1,\r\n",
    "    ):\r\n",
    "        # Same as categorical cross entropy, who would've thought?!\r\n",
    "        if mode == False:\r\n",
    "            return\r\n",
    "        loss_function = nn.NLLLoss()\r\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "        if cutoff:\r\n",
    "            inputs = inputs[:cutoff]\r\n",
    "            outputs = outputs[:cutoff]\r\n",
    "\r\n",
    "        history = {\"loss\": []}\r\n",
    "\r\n",
    "        for epoch in range(n_epochs):\r\n",
    "            counter = 1\r\n",
    "            losses = []\r\n",
    "            for note_series, next_note in zip(inputs, outputs):\r\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\r\n",
    "                # We need to clear them out before each instance\r\n",
    "                self.zero_grad()\r\n",
    "\r\n",
    "                # Step 2. Run our forward pass.\r\n",
    "                c_t = self(note_series)\r\n",
    "\r\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\r\n",
    "                #  calling optimizer.step()\r\n",
    "                loss = loss_function(c_t, next_note.reshape(1).long())\r\n",
    "                loss.backward()\r\n",
    "                optimizer.step()\r\n",
    "                losses.append(float(loss))\r\n",
    "\r\n",
    "                print(f\"On datapoint #{counter} out of {cutoff}\")\r\n",
    "                counter += 1\r\n",
    "\r\n",
    "            avg_loss = np.mean(losses)\r\n",
    "            history[\"loss\"].append(avg_loss)\r\n",
    "            print(\"Epoch {} / {}: Loss = {:.3f}\".format(epoch + 1, n_epochs, avg_loss))\r\n",
    "        return history\r\n",
    "\r\n",
    "    def generate_notes(self, network_input, int_to_note, n_vocab, n_notes):\r\n",
    "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\r\n",
    "        # pick a random sequence from the input as a starting point for the prediction\r\n",
    "        with torch.no_grad():\r\n",
    "            start = np.random.randint(0, len(network_input) - 1)\r\n",
    "\r\n",
    "            pattern = network_input[start]\r\n",
    "            prediction_output = []\r\n",
    "\r\n",
    "            # generate 500 notes\r\n",
    "            for _ in range(n_notes):\r\n",
    "                prediction_input = np.reshape(pattern, (1, len(pattern), 1))\r\n",
    "                # prediction_input = prediction_input / float(n_vocab)\r\n",
    "\r\n",
    "                (h_t, prediction) = self.model.predict(prediction_input)\r\n",
    "\r\n",
    "                index = np.argmax(prediction)\r\n",
    "                result = int_to_note[int(index)]\r\n",
    "                prediction_output.append(result)\r\n",
    "\r\n",
    "                added_index = (index / n_vocab).reshape(1, 1)\r\n",
    "\r\n",
    "                pattern = torch.cat((pattern, added_index), 0)\r\n",
    "                # pattern.append(index)\r\n",
    "                pattern = pattern[1 : len(pattern)]\r\n",
    "\r\n",
    "            return prediction_output\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seq_length = 25\r\n",
    "n_epochs = 10\r\n",
    "cutoff = 20\r\n",
    "n_qubits = 4\r\n",
    "\r\n",
    "model_name = f\"lstm-seq{seq_length}-cut{cutoff}-epcs{n_epochs}-qu{n_qubits}\"\r\n",
    "model_str = f\"saved_models/{model_name}.pt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Initialized Midi\")\r\n",
    "midi = Midi(seq_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Initialized LSTM\")\r\n",
    "lstm = LSTMusic(hidden_dim=midi.n_vocab, n_qubits=n_qubits)\r\n",
    "\r\n",
    "if Path(model_str).is_file():\r\n",
    "    print(\"Loading model\")\r\n",
    "    lstm.load_state_dict(torch.load(model_str))\r\n",
    "    lstm.eval()\r\n",
    "    # lstm = torch.load(model_str)\r\n",
    "else:\r\n",
    "    print(\"Training LSTM\")\r\n",
    "    train_history = lstm.train(\r\n",
    "        True, midi.network_input, midi.network_output, n_epochs=n_epochs, cutoff=cutoff\r\n",
    "    )\r\n",
    "    torch.save(lstm.state_dict(), model_str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Generating notes\")\r\n",
    "notes = lstm.generate_notes(\r\n",
    "    midi.network_input, midi.int_to_note, midi.n_vocab, n_notes=20\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Saving as MIDI file.\")\r\n",
    "midi.create_midi_from_model(notes, f\"generated_songs/{model_name}_generated.mid\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}