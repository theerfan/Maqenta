{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMjfp2zgYszr"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/PennyLaneAI/pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z70RfQb52cPX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.templates import embeddings as emb\n",
        "from pennylane.templates import layers as lay\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "import glob, pickle, time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y11nHDFJ79-C",
        "outputId": "72d48483-8e41-4272-d84b-ffdd4d7bfc30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKEIMOeK4VQV"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/theerfan/Maqenta/raw/main/data/notes.pk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nyTkc6uS2cPb"
      },
      "outputs": [],
      "source": [
        "notes_dir = \"notes.pk\"\n",
        "frequencies_dir = \"freqs.pk\"\n",
        "\n",
        "\n",
        "class Midi:\n",
        "    def __init__(self, seq_length, device):\n",
        "        self.seq_length = seq_length\n",
        "        self.device = device\n",
        "\n",
        "        if Path(notes_dir).is_file():\n",
        "            self.notes = pickle.load(open(notes_dir, \"rb\"))\n",
        "            self.frequencies = pickle.load(open(frequencies_dir, \"rb\"))\n",
        "        else:\n",
        "            self.notes, self.frequencies = self.get_notes()\n",
        "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\n",
        "            pickle.dump(self.frequencies, open(frequencies_dir, \"wb\"))\n",
        "\n",
        "        self.network_input, self.network_output = self.prepare_sequences(self.notes, self.frequencies)\n",
        "        print(f\"Input shape: {self.network_input.shape}\")\n",
        "        print(f\"Output shape: {self.network_output.shape}\")\n",
        "    \n",
        "\n",
        "    def lazy_superimpose(self, input_chord):\n",
        "        frequencies = np.array([note.pitch.frequency for note in input_chord.notes])\n",
        "        return np.average(frequencies, axis=0)\n",
        "\n",
        "\n",
        "    def get_notes(self):\n",
        "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\n",
        "        # This is assuming that every interval between notes is the same (0.5)\n",
        "        notes = []\n",
        "        frequencies = []\n",
        "\n",
        "        for file in glob.glob(\"midi_songs/*.mid\"):\n",
        "            midi = converter.parse(file)\n",
        "\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            notes_to_parse = None\n",
        "\n",
        "            try:  # file has instrument parts\n",
        "                s2 = instrument.partitionByInstrument(midi)\n",
        "                notes_to_parse = s2.parts[0].recurse()\n",
        "            except:  # file has notes in a flat structure\n",
        "                notes_to_parse = midi.flat.notes\n",
        "\n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                    frequencies.append(element.pitch.frequency)\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
        "                    frequencies.append(self.lazy_superimpose(element))\n",
        "\n",
        "        return notes, frequencies\n",
        "\n",
        "    def prepare_sequences(self, notes, frequencies):\n",
        "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\n",
        "        # self.n_vocab = len(set(notes))\n",
        "\n",
        "        # get all pitch names\n",
        "        # pitchnames = sorted(set(item for item in notes))\n",
        "        # Order pitchnames by their frequencies, so that the mse loss makes more sense\n",
        "        pitchnames = list(dict.fromkeys([x for _, x in sorted(zip(frequencies, notes))]))\n",
        "\n",
        "        # create a dictionary to map pitches to integers\n",
        "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
        "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
        "\n",
        "        network_input = []\n",
        "        network_output = []\n",
        "\n",
        "        # create input sequences and the corresponding outputs\n",
        "        for i in range(len(self.notes) - self.seq_length):\n",
        "            sequence_in = self.notes[i : i + self.seq_length]\n",
        "            sequence_out = self.notes[i + self.seq_length]\n",
        "            network_input.append([self.note_to_int[char] for char in sequence_in])\n",
        "            network_output.append(self.note_to_int[sequence_out])\n",
        "\n",
        "        n_patterns = len(network_input)\n",
        "\n",
        "        # reshape the input into a format compatible with LSTM layers\n",
        "        # So this is actuallyt (number of different inputs, sequence length, number of features)\n",
        "        network_input = np.reshape(network_input, (n_patterns, self.seq_length))\n",
        "        network_input = torch.tensor(network_input, device=self.device, dtype=torch.double)\n",
        "\n",
        "        network_output = torch.tensor(network_output, device=self.device)\n",
        "\n",
        "        self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=1))\n",
        "        \n",
        "        return (\n",
        "            network_input,\n",
        "            network_output,\n",
        "        )\n",
        "\n",
        "    def create_midi_from_model(self, prediction_output, filename):\n",
        "        \"\"\"convert the output from the prediction to notes and create a midi file\n",
        "        from the notes\"\"\"\n",
        "        offset = 0\n",
        "        output_notes = []\n",
        "\n",
        "        # create note and chord objects based on the values generated by the model\n",
        "        for pattern in prediction_output:\n",
        "            # pattern is a chord\n",
        "            if (\".\" in pattern) or pattern.isdigit():\n",
        "                notes_in_chord = pattern.split(\".\")\n",
        "                notes = []\n",
        "                for current_note in notes_in_chord:\n",
        "                    new_note = note.Note(int(current_note))\n",
        "                    new_note.storedInstrument = instrument.Piano()\n",
        "                    notes.append(new_note)\n",
        "                new_chord = chord.Chord(notes)\n",
        "                new_chord.offset = offset\n",
        "                output_notes.append(new_chord)\n",
        "            # pattern is a note\n",
        "            else:\n",
        "                new_note = note.Note(pattern)\n",
        "                new_note.offset = offset\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                output_notes.append(new_note)\n",
        "\n",
        "            # increase offset each iteration so that notes do not stack\n",
        "            offset += 0.5\n",
        "\n",
        "        midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "        midi_stream.write(\"midi\", fp=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZIMMZq2j2cPh"
      },
      "outputs": [],
      "source": [
        "# QLSTM.py\n",
        "\n",
        "Embedding = Union[emb.AngleEmbedding, emb.AmplitudeEmbedding, emb.BasisEmbedding]\n",
        "Layer = Union[\n",
        "    lay.BasicEntanglerLayers,\n",
        "    lay.ParticleConservingU1,\n",
        "    lay.ParticleConservingU2,\n",
        "    lay.RandomLayers,\n",
        "    lay.StronglyEntanglingLayers,\n",
        "]\n",
        "\n",
        "\n",
        "class QLSTMCell(nn.Module):\n",
        "    def quantum_op(\n",
        "        self,\n",
        "        wires,\n",
        "        embedding: Embedding = emb.AmplitudeEmbedding,\n",
        "        layer: Layer = lay.RandomLayers,\n",
        "    ):\n",
        "        def circuit_part(inputs, weights):\n",
        "            if embedding == emb.AmplitudeEmbedding:\n",
        "              embedding(inputs.cpu().detach(), wires=wires, normalize=True)\n",
        "            else:\n",
        "              embedding(inputs, wires=wires)\n",
        "            if layer == lay.RandomLayers:\n",
        "              seed = np.random.randint(1, 2**12)\n",
        "              layer(weights, wires=wires, seed=seed)\n",
        "            else:\n",
        "              layer(weights, wires=wires)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in wires]\n",
        "\n",
        "        return circuit_part\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        n_qubits=4,\n",
        "        n_qlayers=1,\n",
        "        dropout=0,\n",
        "        batch_first=True,\n",
        "        return_sequences=False,\n",
        "        return_state=True,\n",
        "        backend=\"default.qubit\",\n",
        "        device=\"cpu\"\n",
        "    ):\n",
        "        super(QLSTMCell, self).__init__()\n",
        "        self.n_inputs = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.concat_size = self.n_inputs + self.hidden_size\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_qlayers = n_qlayers\n",
        "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
        "        self.device = device # \"cpu\", \"cuda\"\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.return_sequences = return_sequences\n",
        "        self.return_state = return_state\n",
        "\n",
        "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
        "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
        "\n",
        "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
        "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
        "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
        "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
        "\n",
        "        self.qlayer_forget = qml.QNode(\n",
        "            self.quantum_op(self.wires_forget), self.dev_forget, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_input = qml.QNode(\n",
        "            self.quantum_op(self.wires_input), self.dev_input, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_update = qml.QNode(\n",
        "            self.quantum_op(self.wires_update), self.dev_update, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        self.qlayer_output = qml.QNode(\n",
        "            self.quantum_op(self.wires_output), self.dev_output, interface=\"torch\"\n",
        "        )\n",
        "\n",
        "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
        "        # print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
        "\n",
        "        self.VQC_forget = qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes).to(device)\n",
        "        self.VQC_input = qml.qnn.TorchLayer(self.qlayer_input, weight_shapes).to(device)\n",
        "        self.VQC_update = qml.qnn.TorchLayer(self.qlayer_update, weight_shapes).to(device)\n",
        "        self.VQC_output = qml.qnn.TorchLayer(self.qlayer_output, weight_shapes).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        \"\"\"\n",
        "        x.shape is (batch_size, seq_length, feature_size)\n",
        "        recurrent_activation -> sigmoid\n",
        "        activation -> tanh\n",
        "        \"\"\"\n",
        "        # Automatically assumes single batch\n",
        "\n",
        "        x = x.to(self.device)\n",
        "        seq_length = x.size()\n",
        "\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t = torch.zeros(self.hidden_size, device=self.device) # hidden state (output)\n",
        "            c_t = torch.zeros(self.hidden_size, device=self.device) # cell state\n",
        "        else:\n",
        "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
        "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
        "            h_t, c_t = init_states\n",
        "            # h_t = h_t[0]\n",
        "            # c_t = c_t[0]\n",
        "\n",
        "        # Concatenate input and hidden state\n",
        "        y_t = torch.cat((h_t, x), dim=0).float().to(device)\n",
        "        \n",
        "        f_t = torch.sigmoid(self.VQC_forget(y_t).to(self.device))  # forget block\n",
        "        i_t = torch.sigmoid(self.VQC_input(y_t).to(self.device)) # input block\n",
        "        g_t = torch.tanh(self.VQC_update(y_t).to(self.device))  # update block\n",
        "        o_t = torch.sigmoid(self.VQC_output(y_t).to(self.device)) # output block\n",
        "\n",
        "        c_t = (f_t * c_t) + (i_t * g_t)\n",
        "        h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "        hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        h_t, c_t = h_t.float(), c_t.float()\n",
        "\n",
        "        if self.dropout:\n",
        "          F.dropout(h_t, self.dropout, inplace=True)\n",
        "\n",
        "        if self.return_state:\n",
        "            if self.return_sequences:\n",
        "                return hidden_seq, (h_t, c_t)\n",
        "            else:\n",
        "                return (h_t, c_t)\n",
        "        else:\n",
        "            if self.return_sequences:\n",
        "                return hidden_seq\n",
        "            else:\n",
        "                return h_t\n",
        "    \n",
        "    def predict(self, x, init_states=None):\n",
        "        return self.forward(x, init_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DE5yYIB3XKu3"
      },
      "outputs": [],
      "source": [
        "class QLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size: int, hidden_size: int, n_layers: int, n_qubits: list, n_qlayers: list, dropouts: list, backend=\"default.qubit\", device=\"cpu\"):\n",
        "    super(QLSTM, self).__init__()\n",
        "    self.models = nn.ModuleList()\n",
        "    self.n_layers = n_layers\n",
        "    for i in range(self.n_layers):\n",
        "      self.models.append(\n",
        "          QLSTMCell(input_size, hidden_size, n_qubits[i], n_qlayers[i], dropouts[i], backend=backend, device=device)\n",
        "      )\n",
        "    \n",
        "  def forward(self, note_sequences):\n",
        "    # A tuple of (h_t, c_t)\n",
        "    outputs = []\n",
        "    h_t_c_t = None\n",
        "    for i in range(self.n_layers):\n",
        "      h_t_c_t = self.models[i](note_sequences[i], h_t_c_t)\n",
        "      # Only output c_t\n",
        "      outputs.append(h_t_c_t[1])\n",
        "    \n",
        "    return torch.stack(outputs)\n",
        "\n",
        "  def predict(self, note_sequences):\n",
        "    return self.forward(note_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Cg9XF7zHCKZe"
      },
      "outputs": [],
      "source": [
        "def post_processing(ct_list, starting_i):\n",
        "  i = starting_i\n",
        "  for j in range(starting_i, starting_i + ct_list.shape[0]):\n",
        "    ct_list[j-i] = (ct_list[j-i] * float(midi.input_norms[j])).long()\n",
        "    ct_list[j-i] = torch.mean(ct_list[j-i]).long()\n",
        "  \n",
        "  return ct_list[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3AgEdPUc2cPk"
      },
      "outputs": [],
      "source": [
        "# LSTMusic.py\n",
        "\n",
        "class QLSTMusic(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_qlayers=1,\n",
        "        n_layers=1,\n",
        "        dropout=0.3,\n",
        "        n_vocab=None,\n",
        "        input_dim=1,\n",
        "        hidden_dim=512,\n",
        "        n_qubits=4,\n",
        "        backend=\"default.qubit\",\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        super(QLSTMusic, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        if n_qubits > 0:\n",
        "            print(f\"Generator will use Quantum LSTM running on backend {backend}\")\n",
        "            r_n = range(n_layers)\n",
        "            n_qubits = [n_qubits for _ in r_n]\n",
        "            n_qlayers = [n_qlayers for _ in r_n]\n",
        "            dropouts = [dropout for _ in r_n]\n",
        "            self.model = QLSTM(input_dim, hidden_dim, n_layers, n_qubits, n_qlayers, dropouts, device=device).to(device)\n",
        "        else:\n",
        "            print(\"Generator will use Classical LSTM\")\n",
        "            self.model = nn.LSTM(input_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "\n",
        "    def forward(self, note_sequences):\n",
        "        ct_list = self.model(note_sequences)\n",
        "        return torch.abs(ct_list)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        mode=True,\n",
        "        inputs=None,\n",
        "        outputs=None,\n",
        "        n_epochs=None,\n",
        "        cutoff: int = None,\n",
        "        learning_rate=0.1,\n",
        "    ):\n",
        "        if mode == False:\n",
        "            return\n",
        "        loss_function = nn.MSELoss()\n",
        "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()), lr=learning_rate)\n",
        "\n",
        "        if cutoff:\n",
        "            inputs = inputs[:cutoff]\n",
        "            outputs = outputs[:cutoff]\n",
        "\n",
        "        history = {\"loss\": []}\n",
        "\n",
        "        midi_data = list(zip(inputs, outputs))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            counter = 0\n",
        "            losses = []\n",
        "\n",
        "            for i in range(0, len(midi_data) - self.n_layers, self.n_layers):\n",
        "              data = midi_data[i:i+self.n_layers]\n",
        "              midi_input_sequences = [datum[0] for datum in data]\n",
        "              midi_output_sequences = torch.stack([datum[1] for datum in data])\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              lstm_output_sequence = self(midi_input_sequences)\n",
        "              lstm_output_sequence = post_processing(lstm_output_sequence, i)\n",
        "\n",
        "              loss = loss_function(lstm_output_sequence, midi_output_sequences.float())\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              losses.append(float(loss))\n",
        "\n",
        "              if counter % 5 == 0:\n",
        "                print(f\"On datapoint #{counter} out of {cutoff}\")\n",
        "              counter += 1\n",
        "\n",
        "            avg_loss = np.mean(losses)\n",
        "            history[\"loss\"].append(avg_loss)\n",
        "            print(\"Epoch {} / {}: Loss = {:.3f}\".format(epoch + 1, n_epochs, avg_loss))\n",
        "        return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j0r6iXZ82cPm"
      },
      "outputs": [],
      "source": [
        "n_qlayers = 2\n",
        "n_qubits = 6\n",
        "seq_length = 2**n_qubits - n_qubits\n",
        "\n",
        "n_layers = 50\n",
        "\n",
        "n_epochs = 3\n",
        "cutoff = 10_000\n",
        "\n",
        "generator_counter = 0\n",
        "\n",
        "model_name = f\"lstm{n_layers}-seq{seq_length}-cut{cutoff}-epcs{n_epochs}-qu{n_qubits}-nq{n_qlayers}\"\n",
        "model_str = f\"{model_name}.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a33Nq6eh2cPm",
        "outputId": "c9954c81-0874-43d2-cc85-42ea82a8552d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Midi\n",
            "Input shape: torch.Size([0, 58])\n",
            "Output shape: torch.Size([0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\TheRe\\AppData\\Local\\Temp/ipykernel_31500/195409743.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=1))\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing Midi\")\n",
        "midi = Midi(seq_length, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LafpbTRVYeE-",
        "outputId": "3e7bd5f1-5687-48e8-9e4d-109293d5e6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized LSTM\n",
            "Generator will use Quantum LSTM running on backend default.qubit\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing LSTM\")\n",
        "lstm = QLSTMusic(n_qubits=n_qubits, n_qlayers=n_qlayers, n_layers=n_layers, hidden_dim=n_qubits, device=device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sJM8dALYjfE"
      },
      "outputs": [],
      "source": [
        "def print_parameters():\n",
        "  for name, param in lstm.named_parameters():\n",
        "      if param.requires_grad:\n",
        "          print(name, param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rkZC4Yb2cPn"
      },
      "outputs": [],
      "source": [
        "# TODO: Separate input data to test/train\n",
        "\n",
        "if Path(model_str).is_file():\n",
        "    print(\"Loading model\")\n",
        "    lstm.load_state_dict(torch.load(model_str))\n",
        "    lstm.eval()\n",
        "    # lstm = torch.load(model_str)\n",
        "else:\n",
        "    print(\"Training LSTM\")\n",
        "    train_history = lstm.train(\n",
        "        True, midi.network_input, midi.network_output, n_epochs=n_epochs, cutoff=cutoff\n",
        "    )\n",
        "    torch.save(lstm.state_dict(), model_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JTxPm8XrDuLa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_notes(self, network_input, int_to_note, n_notes, n_layers):\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
        "        # pick a random sequence from the input as a starting point for the prediction\n",
        "        req_size = n_notes//n_layers\n",
        "        start = random.randint(0, len(network_input) - req_size)\n",
        "        prediction_output = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # generate n_notes\n",
        "            for i in range(start, start + n_notes, n_layers):\n",
        "                prediction_input = network_input[i:i+n_layers]\n",
        "\n",
        "                model_output = self.forward(prediction_input)\n",
        "                model_output = post_processing(model_output, i)\n",
        "\n",
        "                for prediction in model_output:\n",
        "                  index = torch.mean(prediction)\n",
        "                  result = int_to_note.get(int(index), '0')\n",
        "                  prediction_output.append(result)\n",
        "\n",
        "            return prediction_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7PVNr3_2cPo"
      },
      "outputs": [],
      "source": [
        "n_notes = 200\n",
        "print(\"Generating notes\")\n",
        "notes = generate_notes(\n",
        "    lstm, midi.network_input, midi.int_to_note, n_notes=n_notes, n_layers=n_layers\n",
        ")\n",
        "notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChgF6LoQ2cPq",
        "outputId": "b7d5610d-542b-4a28-f686-980e33910a30"
      },
      "outputs": [],
      "source": [
        "generator_counter += 1\n",
        "print(\"Saving as MIDI file.\")\n",
        "midi.create_midi_from_model(notes, f\"{model_name}_generated_{generator_counter}.mid\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Copy of main_gpu.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "dc44aa18a01d4ef3c49dce97499f9883b1b573f96a33e2e79347f993e3562639"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
