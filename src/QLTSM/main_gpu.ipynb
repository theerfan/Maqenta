{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of main_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('venv': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "interpreter": {
      "hash": "dc44aa18a01d4ef3c49dce97499f9883b1b573f96a33e2e79347f993e3562639"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install git+https://github.com/PennyLaneAI/pennylane"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PennyLaneAI/pennylane\n",
            "  Cloning https://github.com/PennyLaneAI/pennylane to /tmp/pip-req-build-ngjy5xfs\n",
            "  Running command git clone -q https://github.com/PennyLaneAI/pennylane /tmp/pip-req-build-ngjy5xfs\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (2.6.3)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (1.4.4)\n",
            "Collecting semantic_version==2.6\n",
            "  Downloading semantic_version-2.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting autoray\n",
            "  Downloading autoray-0.2.5-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from PennyLane==0.19.0.dev0) (4.2.4)\n",
            "Collecting pennylane-lightning>=0.18\n",
            "  Downloading PennyLane_Lightning-0.18.0-cp37-cp37m-manylinux2010_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd->PennyLane==0.19.0.dev0) (0.16.0)\n",
            "Building wheels for collected packages: PennyLane\n",
            "  Building wheel for PennyLane (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PennyLane: filename=PennyLane-0.19.0.dev0-py3-none-any.whl size=674636 sha256=e61a1e64a9d95591e41a7c7ba91744614938c7128e2f8a25abf10e9af9caecb5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5yrt0zp1/wheels/9e/4b/fe/27dcf8ba174161f9d1af1841251dc97013a33a66f16cd8d661\n",
            "Successfully built PennyLane\n",
            "Installing collected packages: semantic-version, pennylane-lightning, autoray, PennyLane\n",
            "Successfully installed PennyLane-0.19.0.dev0 autoray-0.2.5 pennylane-lightning-0.18.0 semantic-version-2.6.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "MMjfp2zgYszr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765b3ca4-7fc0-4f25-d9b5-9f58e1b953d3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import tensor\r\n",
        "\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "import pennylane as qml\r\n",
        "from pennylane import numpy as np\r\n",
        "from pennylane.templates import embeddings as emb\r\n",
        "from pennylane.templates import layers as lay\r\n",
        "\r\n",
        "from typing import Union\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from music21 import converter, instrument, note, chord, stream\r\n",
        "import glob, pickle, time"
      ],
      "outputs": [],
      "metadata": {
        "id": "z70RfQb52cPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y11nHDFJ79-C",
        "outputId": "ca6951cf-5c4d-4a29-8cb1-314c3cdfd354"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "!wget https://github.com/theerfan/Maqenta/raw/main/data/notes.pk"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKEIMOeK4VQV",
        "outputId": "1970df71-aceb-44d8-d999-5d18140d7471"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Midi.py\r\n",
        "\r\n",
        "notes_dir = \"notes.pk\"\r\n",
        "\r\n",
        "\r\n",
        "class Midi:\r\n",
        "    def __init__(self, seq_length, device):\r\n",
        "        self.seq_length = seq_length\r\n",
        "        self.device = device\r\n",
        "\r\n",
        "        if Path(notes_dir).is_file():\r\n",
        "            self.notes = pickle.load(open(notes_dir, \"rb\"))\r\n",
        "            # self.notes = pickle.loads(uploaded[notes_dir])\r\n",
        "        else:\r\n",
        "            self.notes = self.get_notes()\r\n",
        "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\r\n",
        "\r\n",
        "        self.network_input, self.network_output = self.prepare_sequences(self.notes)\r\n",
        "        print(f\"Input shape: {self.network_input.shape}\")\r\n",
        "        print(f\"Output shape: {self.network_output.shape}\")\r\n",
        "\r\n",
        "    def get_notes(self):\r\n",
        "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\r\n",
        "        # This is assuming that every interval between notes is the same (0.5)\r\n",
        "        notes = []\r\n",
        "\r\n",
        "        for file in glob.glob(\"midi_songs/*.mid\"):\r\n",
        "            midi = converter.parse(file)\r\n",
        "\r\n",
        "            print(\"Parsing %s\" % file)\r\n",
        "\r\n",
        "            notes_to_parse = None\r\n",
        "\r\n",
        "            try:  # file has instrument parts\r\n",
        "                s2 = instrument.partitionByInstrument(midi)\r\n",
        "                notes_to_parse = s2.parts[0].recurse()\r\n",
        "            except:  # file has notes in a flat structure\r\n",
        "                notes_to_parse = midi.flat.notes\r\n",
        "\r\n",
        "            for element in notes_to_parse:\r\n",
        "                if isinstance(element, note.Note):\r\n",
        "                    notes.append(str(element.pitch))\r\n",
        "                elif isinstance(element, chord.Chord):\r\n",
        "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\r\n",
        "\r\n",
        "        with open(notes_dir, \"wb\") as filepath:\r\n",
        "            pickle.dump(notes, filepath)\r\n",
        "\r\n",
        "        return notes\r\n",
        "\r\n",
        "    def prepare_sequences(self, notes):\r\n",
        "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\r\n",
        "        self.n_vocab = len(set(notes))\r\n",
        "\r\n",
        "        # get all pitch names\r\n",
        "        pitchnames = sorted(set(item for item in notes))\r\n",
        "\r\n",
        "        # create a dictionary to map pitches to integers\r\n",
        "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\r\n",
        "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\r\n",
        "\r\n",
        "        network_input = []\r\n",
        "        network_output = []\r\n",
        "\r\n",
        "        # create input sequences and the corresponding outputs\r\n",
        "        for i in range(len(self.notes) - self.seq_length):\r\n",
        "            sequence_in = self.notes[i : i + self.seq_length]\r\n",
        "            sequence_out = self.notes[i + self.seq_length]\r\n",
        "            network_input.append([self.note_to_int[char] for char in sequence_in])\r\n",
        "            network_output.append(self.note_to_int[sequence_out])\r\n",
        "\r\n",
        "        n_patterns = len(network_input)\r\n",
        "\r\n",
        "        # reshape the input into a format compatible with LSTM layers\r\n",
        "        # So this is actuallyt (number of different inputs, sequence length, number of features)\r\n",
        "        network_input = np.reshape(network_input, (n_patterns, self.seq_length))\r\n",
        "        network_input = torch.tensor(network_input, device=self.device, dtype=torch.double)\r\n",
        "\r\n",
        "        self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=1))\r\n",
        "        \r\n",
        "        # print(network_input.shape)\r\n",
        "        for i in range(network_input.shape[0]):\r\n",
        "            network_input[i] /= self.input_norms[i]\r\n",
        "        # network_input = torch.div(network_input, self.input_norms)\r\n",
        "\r\n",
        "        return (\r\n",
        "            network_input,\r\n",
        "            torch.tensor(network_output, device=self.device),\r\n",
        "        )\r\n",
        "\r\n",
        "    def create_midi_from_model(self, prediction_output, filename):\r\n",
        "        \"\"\"convert the output from the prediction to notes and create a midi file\r\n",
        "        from the notes\"\"\"\r\n",
        "        offset = 0\r\n",
        "        output_notes = []\r\n",
        "\r\n",
        "        # create note and chord objects based on the values generated by the model\r\n",
        "        for pattern in prediction_output:\r\n",
        "            # pattern is a chord\r\n",
        "            if (\".\" in pattern) or pattern.isdigit():\r\n",
        "                notes_in_chord = pattern.split(\".\")\r\n",
        "                notes = []\r\n",
        "                for current_note in notes_in_chord:\r\n",
        "                    new_note = note.Note(int(current_note))\r\n",
        "                    new_note.storedInstrument = instrument.Piano()\r\n",
        "                    notes.append(new_note)\r\n",
        "                new_chord = chord.Chord(notes)\r\n",
        "                new_chord.offset = offset\r\n",
        "                output_notes.append(new_chord)\r\n",
        "            # pattern is a note\r\n",
        "            else:\r\n",
        "                new_note = note.Note(pattern)\r\n",
        "                new_note.offset = offset\r\n",
        "                new_note.storedInstrument = instrument.Piano()\r\n",
        "                output_notes.append(new_note)\r\n",
        "\r\n",
        "            # increase offset each iteration so that notes do not stack\r\n",
        "            offset += 0.5\r\n",
        "\r\n",
        "        midi_stream = stream.Stream(output_notes)\r\n",
        "\r\n",
        "        midi_stream.write(\"midi\", fp=filename)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nyTkc6uS2cPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# QLSTM.py\r\n",
        "\r\n",
        "Embedding = Union[emb.AngleEmbedding, emb.AmplitudeEmbedding, emb.BasisEmbedding]\r\n",
        "Layer = Union[\r\n",
        "    lay.BasicEntanglerLayers,\r\n",
        "    lay.ParticleConservingU1,\r\n",
        "    lay.ParticleConservingU2,\r\n",
        "    lay.RandomLayers,\r\n",
        "    lay.StronglyEntanglingLayers,\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "class QLSTMCell(nn.Module):\r\n",
        "    def quantum_op(\r\n",
        "        self,\r\n",
        "        wires,\r\n",
        "        embedding: Embedding = emb.AmplitudeEmbedding,\r\n",
        "        layer: Layer = lay.RandomLayers,\r\n",
        "    ):\r\n",
        "        def circuit_part(inputs, weights):\r\n",
        "            if embedding == emb.AmplitudeEmbedding:\r\n",
        "              embedding(inputs.cpu().detach(), wires=wires, normalize=True)\r\n",
        "            else:\r\n",
        "              embedding(inputs, wires=wires)\r\n",
        "            if layer == lay.RandomLayers:\r\n",
        "              seed = np.random.randint(1, 2**12)\r\n",
        "              layer(weights, wires=wires, seed=seed)\r\n",
        "            else:\r\n",
        "              layer(weights, wires=wires)\r\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in wires]\r\n",
        "\r\n",
        "        return circuit_part\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        input_size,\r\n",
        "        hidden_size,\r\n",
        "        n_qubits=4,\r\n",
        "        n_qlayers=1,\r\n",
        "        dropout=0,\r\n",
        "        batch_first=True,\r\n",
        "        return_sequences=False,\r\n",
        "        return_state=True,\r\n",
        "        backend=\"default.qubit\",\r\n",
        "        device=\"cpu\"\r\n",
        "    ):\r\n",
        "        super(QLSTMCell, self).__init__()\r\n",
        "        self.n_inputs = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.concat_size = self.n_inputs + self.hidden_size\r\n",
        "        self.n_qubits = n_qubits\r\n",
        "        self.n_qlayers = n_qlayers\r\n",
        "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\r\n",
        "        self.device = device # \"cpu\", \"cuda\"\r\n",
        "        self.dropout = dropout\r\n",
        "\r\n",
        "        self.batch_first = batch_first\r\n",
        "        self.return_sequences = return_sequences\r\n",
        "        self.return_state = return_state\r\n",
        "\r\n",
        "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\r\n",
        "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\r\n",
        "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\r\n",
        "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\r\n",
        "\r\n",
        "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\r\n",
        "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\r\n",
        "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\r\n",
        "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\r\n",
        "\r\n",
        "        self.qlayer_forget = qml.QNode(\r\n",
        "            self.quantum_op(self.wires_forget), self.dev_forget, interface=\"torch\"\r\n",
        "        )\r\n",
        "\r\n",
        "        self.qlayer_input = qml.QNode(\r\n",
        "            self.quantum_op(self.wires_input), self.dev_input, interface=\"torch\"\r\n",
        "        )\r\n",
        "\r\n",
        "        self.qlayer_update = qml.QNode(\r\n",
        "            self.quantum_op(self.wires_update), self.dev_update, interface=\"torch\"\r\n",
        "        )\r\n",
        "\r\n",
        "        self.qlayer_output = qml.QNode(\r\n",
        "            self.quantum_op(self.wires_output), self.dev_output, interface=\"torch\"\r\n",
        "        )\r\n",
        "\r\n",
        "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\r\n",
        "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\r\n",
        "\r\n",
        "        # self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\r\n",
        "        self.VQC_forget = qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes).to(device)\r\n",
        "        self.VQC_input = qml.qnn.TorchLayer(self.qlayer_input, weight_shapes).to(device)\r\n",
        "        self.VQC_update = qml.qnn.TorchLayer(self.qlayer_update, weight_shapes).to(device)\r\n",
        "        self.VQC_output = qml.qnn.TorchLayer(self.qlayer_output, weight_shapes).to(device)\r\n",
        "\r\n",
        "        # self.VQC = {\r\n",
        "        #     \"forget\": qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes).to(device),\r\n",
        "        #     \"input\": qml.qnn.TorchLayer(self.qlayer_input, weight_shapes).to(device),\r\n",
        "        #     \"update\": qml.qnn.TorchLayer(self.qlayer_update, weight_shapes).to(device),\r\n",
        "        #     \"output\": qml.qnn.TorchLayer(self.qlayer_output, weight_shapes).to(device),\r\n",
        "        # }\r\n",
        "        # self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\r\n",
        "\r\n",
        "        # Turn off classical parameters to see if they're the problem\r\n",
        "        # for out_param in self.clayer_out.parameters():\r\n",
        "        #   out_param.requires_grad = False\r\n",
        "        # for in_param in self.clayer_in.parameters():\r\n",
        "        #   in_param.requires_grad = False\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, init_states=None):\r\n",
        "        \"\"\"\r\n",
        "        x.shape is (batch_size, seq_length, feature_size)\r\n",
        "        recurrent_activation -> sigmoid\r\n",
        "        activation -> tanh\r\n",
        "        \"\"\"\r\n",
        "        # Automatically assumes single batch\r\n",
        "        x = x.to(self.device)\r\n",
        "        # if len(x.shape) == 2:\r\n",
        "            # x = x.reshape(1, x.shape[0], x.shape[1])\r\n",
        "        seq_length = x.size()\r\n",
        "        \r\n",
        "        # if self.batch_first is True:\r\n",
        "            # batch_size, seq_length, features_size = x.size()\r\n",
        "        # else:\r\n",
        "            # seq_length, batch_size, features_size = x.size()\r\n",
        "\r\n",
        "        hidden_seq = []\r\n",
        "        if init_states is None:\r\n",
        "            h_t = torch.zeros(self.hidden_size, device=self.device) # hidden state (output)\r\n",
        "            c_t = torch.zeros(self.hidden_size, device=self.device) # cell state\r\n",
        "        else:\r\n",
        "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\r\n",
        "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\r\n",
        "            h_t, c_t = init_states\r\n",
        "            # h_t = h_t[0]\r\n",
        "            # c_t = c_t[0]\r\n",
        "\r\n",
        "        # for t in range(seq_length):\r\n",
        "            # get features from the t-th element in seq, for all entries in the batch\r\n",
        "            # x_t = x[:, t, :]\r\n",
        "\r\n",
        "            # Concatenate input and hidden state\r\n",
        "        y_t = torch.cat((h_t, x), dim=0).float().to(device)\r\n",
        "\r\n",
        "        # match qubit dimension\r\n",
        "        # y_t = self.clayer_in(v_t).to(self.device)\r\n",
        "        \r\n",
        "        f_t = torch.sigmoid(self.VQC_forget(y_t).to(self.device))  # forget block\r\n",
        "        i_t = torch.sigmoid(self.VQC_input(y_t).to(self.device)) # input block\r\n",
        "        g_t = torch.tanh(self.VQC_update(y_t).to(self.device))  # update block\r\n",
        "        o_t = torch.sigmoid(self.VQC_output(y_t).to(self.device))\r\n",
        "        # ).to(self.device)  # output block\r\n",
        "\r\n",
        "        c_t = (f_t * c_t) + (i_t * g_t)\r\n",
        "        h_t = o_t * torch.tanh(c_t)\r\n",
        "\r\n",
        "        hidden_seq.append(h_t.unsqueeze(0))\r\n",
        "\r\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\r\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\r\n",
        "\r\n",
        "        h_t, c_t = h_t.float(), c_t.float()\r\n",
        "\r\n",
        "        if self.dropout:\r\n",
        "          F.dropout(h_t, self.dropout, inplace=True)\r\n",
        "\r\n",
        "        if self.return_state:\r\n",
        "            if self.return_sequences:\r\n",
        "                return hidden_seq, (h_t, c_t)\r\n",
        "            else:\r\n",
        "                return (h_t, c_t)\r\n",
        "        else:\r\n",
        "            if self.return_sequences:\r\n",
        "                return hidden_seq\r\n",
        "            else:\r\n",
        "                return h_t\r\n",
        "    \r\n",
        "    def predict(self, x, init_states=None):\r\n",
        "        return self.forward(x, init_states)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZIMMZq2j2cPh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "class QLSTM(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, input_size: int, hidden_size: int, n_layers: int, n_qubits: list, n_qlayers: list, dropouts: list, backend=\"default.qubit\", device=\"cpu\"):\r\n",
        "    super(QLSTM, self).__init__()\r\n",
        "    self.models = nn.ModuleList()\r\n",
        "    self.n_layers = n_layers\r\n",
        "    for i in range(self.n_layers):\r\n",
        "      self.models.append(\r\n",
        "          QLSTMCell(input_size, hidden_size, n_qubits[i], n_qlayers[i], dropouts[i], backend=backend, device=device)\r\n",
        "      )\r\n",
        "    \r\n",
        "  def forward(self, note_sequences):\r\n",
        "    # A tuple of (h_t, c_t)\r\n",
        "    outputs = []\r\n",
        "    h_t_c_t = None\r\n",
        "    for i in range(self.n_layers):\r\n",
        "      h_t_c_t = self.models[i](note_sequences[i], h_t_c_t)\r\n",
        "      # Only output c_t\r\n",
        "      outputs.append(h_t_c_t[1])\r\n",
        "    \r\n",
        "    # print(outputs)\r\n",
        "    return torch.stack(outputs)\r\n",
        "\r\n",
        "  def predict(self, note_sequences):\r\n",
        "    return self.forward(note_sequences)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "DE5yYIB3XKu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "def post_processing(ct_list, starting_i):\r\n",
        "  i = starting_i\r\n",
        "  for j in range(starting_i, starting_i + ct_list.shape[0]):\r\n",
        "    ct_list[j-i] = (ct_list[j-i] * float(midi.input_norms[j])).long()\r\n",
        "    ct_list[j-i] = torch.mean(ct_list[j-i]).long()\r\n",
        "  \r\n",
        "  return ct_list[:, 0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "Cg9XF7zHCKZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "# LSTMusic.py\r\n",
        "\r\n",
        "# import torch\r\n",
        "# import torch.nn as nn\r\n",
        "# from torch import optim\r\n",
        "# import torch.nn.functional as F\r\n",
        "\r\n",
        "# from QLTSM.qlstm import QLSTM\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "class LSTMusic(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        n_qlayers=1,\r\n",
        "        n_layers=1,\r\n",
        "        dropout=0.3,\r\n",
        "        n_vocab=None,\r\n",
        "        input_dim=1,\r\n",
        "        hidden_dim=512,\r\n",
        "        n_qubits=4,\r\n",
        "        backend=\"default.qubit\",\r\n",
        "        device=\"cpu\",\r\n",
        "    ):\r\n",
        "        super(LSTMusic, self).__init__()\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.n_layers = n_layers\r\n",
        "\r\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n",
        "        # with dimensionality hidden_dim.\r\n",
        "        if n_qubits > 0:\r\n",
        "            print(f\"Generator will use Quantum LSTM running on backend {backend}\")\r\n",
        "            r_n = range(n_layers)\r\n",
        "            n_qubits = [n_qubits for _ in r_n]\r\n",
        "            n_qlayers = [n_qlayers for _ in r_n]\r\n",
        "            dropouts = [dropout for _ in r_n]\r\n",
        "            self.model = QLSTM(input_dim, hidden_dim, n_layers, n_qubits, n_qlayers, dropouts, device=device).to(device)\r\n",
        "        else:\r\n",
        "            print(\"Generator will use Classical LSTM\")\r\n",
        "            self.model = nn.LSTM(input_dim, hidden_dim)\r\n",
        "\r\n",
        "        # The linear layer that maps from hidden state space to tag space\r\n",
        "\r\n",
        "    def forward(self, note_sequences):\r\n",
        "        ct_list = self.model(note_sequences)\r\n",
        "        return torch.abs(ct_list)\r\n",
        "        # scores = []\r\n",
        "        # for c_t in ct_list:\r\n",
        "          # scores.append(F.log_softmax(c_t, dim=1))\r\n",
        "        # ct_list = torch.stack(scores)\r\n",
        "        # return ct_list.reshape(ct_list.shape[0], ct_list.shape[2])\r\n",
        "\r\n",
        "    def predict(self, note_sequences):\r\n",
        "        return self.forward(note_sequences)\r\n",
        "\r\n",
        "    def train(\r\n",
        "        self,\r\n",
        "        mode=True,\r\n",
        "        inputs=None,\r\n",
        "        outputs=None,\r\n",
        "        n_epochs=None,\r\n",
        "        cutoff: int = None,\r\n",
        "        learning_rate=0.1,\r\n",
        "    ):\r\n",
        "        # Same as categorical cross entropy, who would've thought?!\r\n",
        "        if mode == False:\r\n",
        "            return\r\n",
        "        loss_function = nn.MSELoss()\r\n",
        "        # loss_function = nn.NLLLoss()\r\n",
        "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()), lr=learning_rate)\r\n",
        "\r\n",
        "        if cutoff:\r\n",
        "            inputs = inputs[:cutoff]\r\n",
        "            outputs = outputs[:cutoff]\r\n",
        "\r\n",
        "        history = {\"loss\": []}\r\n",
        "\r\n",
        "        midi_data = list(zip(inputs, outputs))\r\n",
        "\r\n",
        "        for epoch in range(n_epochs):\r\n",
        "            counter = 0\r\n",
        "            losses = []\r\n",
        "\r\n",
        "            for i in range(0, len(midi_data) - self.n_layers, self.n_layers):\r\n",
        "              data = midi_data[i:i+self.n_layers]\r\n",
        "              note_seqs = [datum[0] for datum in data]\r\n",
        "              next_notes = torch.stack([datum[1] for datum in data])\r\n",
        "\r\n",
        "              optimizer.zero_grad()\r\n",
        "              c_t_list = self(note_seqs)\r\n",
        "              c_t_list = post_processing(c_t_list, i)\r\n",
        "\r\n",
        "              loss = loss_function(c_t_list, next_notes.float())\r\n",
        "              loss.backward()\r\n",
        "              optimizer.step()\r\n",
        "              losses.append(float(loss))\r\n",
        "\r\n",
        "              if counter % 5 == 0:\r\n",
        "                print(f\"On datapoint #{counter} out of {cutoff}\")\r\n",
        "              counter += 1\r\n",
        "\r\n",
        "            avg_loss = np.mean(losses)\r\n",
        "            history[\"loss\"].append(avg_loss)\r\n",
        "            print(\"Epoch {} / {}: Loss = {:.3f}\".format(epoch + 1, n_epochs, avg_loss))\r\n",
        "        return history"
      ],
      "outputs": [],
      "metadata": {
        "id": "3AgEdPUc2cPk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "n_qubits = 6\r\n",
        "seq_length = 2**n_qubits - n_qubits\r\n",
        "n_epochs = 1\r\n",
        "cutoff = 200\r\n",
        "n_layers = 4\r\n",
        "n_qlayers = 2\r\n",
        "\r\n",
        "model_name = f\"lstm{n_layers}-seq{seq_length}-cut{cutoff}-epcs{n_epochs}-qu{n_qubits}-nq{n_qlayers}\"\r\n",
        "model_str = f\"{model_name}.pt\""
      ],
      "outputs": [],
      "metadata": {
        "id": "j0r6iXZ82cPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "print(\"Initialized Midi\")\r\n",
        "midi = Midi(seq_length, device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Midi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\TheRe\\AppData\\Local\\Temp/ipykernel_2796/440410498.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([44798, 58])\n",
            "Output shape: torch.Size([44798])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a33Nq6eh2cPm",
        "outputId": "97cd050a-9ed6-4389-d9a2-2b0bae5ad8c8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "print(\"Initialized LSTM\")\r\n",
        "lstm = LSTMusic(n_qubits=n_qubits, n_qlayers=n_qlayers, n_layers=n_layers, hidden_dim=n_qubits, device=device).to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized LSTM\n",
            "Generator will use Quantum LSTM running on backend default.qubit\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 6)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 6)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 6)\n",
            "weight_shapes = (n_qlayers, n_qubits) = (2, 6)\n"
          ]
        }
      ],
      "metadata": {
        "id": "LafpbTRVYeE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4999ed-5e1b-4ad0-98b4-c6599f6eef4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# for param in lstm.model.models[0].clayer_in.parameters():\r\n",
        "#   print(param.requires_grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "metadata": {
        "id": "DWXB84ccy4kz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc3d330-8d61-4856-a5bd-d9c14172c380"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "def print_parameters():\r\n",
        "  for name, param in lstm.named_parameters():\r\n",
        "      if param.requires_grad:\r\n",
        "          print(name, param.data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7sJM8dALYjfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "# TODO: Separate input data to test/train\r\n",
        "\r\n",
        "if Path(model_str).is_file():\r\n",
        "    print(\"Loading model\")\r\n",
        "    lstm.load_state_dict(torch.load(model_str))\r\n",
        "    lstm.eval()\r\n",
        "    # lstm = torch.load(model_str)\r\n",
        "else:\r\n",
        "    print(\"Training LSTM\")\r\n",
        "    train_history = lstm.train(\r\n",
        "        True, midi.network_input, midi.network_output, n_epochs=n_epochs, cutoff=cutoff\r\n",
        "    )\r\n",
        "    torch.save(lstm.state_dict(), model_str)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM\n",
            "On datapoint #0 out of 200\n",
            "On datapoint #5 out of 200\n",
            "On datapoint #10 out of 200\n",
            "On datapoint #15 out of 200\n",
            "On datapoint #20 out of 200\n",
            "On datapoint #25 out of 200\n",
            "On datapoint #30 out of 200\n",
            "On datapoint #35 out of 200\n",
            "On datapoint #40 out of 200\n",
            "On datapoint #45 out of 200\n",
            "Epoch 1 / 1: Loss = 11886.245\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "-rkZC4Yb2cPn",
        "outputId": "6e01fd01-b101-423a-bbf1-789437609b7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "source": [
        "def generate_notes(self, network_input, int_to_note, n_vocab, n_notes, n_layers):\r\n",
        "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\r\n",
        "        # pick a random sequence from the input as a starting point for the prediction\r\n",
        "        with torch.no_grad():\r\n",
        "            req_size = n_notes//n_layers\r\n",
        "            start = np.random.randint(0, len(network_input) - req_size)\r\n",
        "\r\n",
        "            # pattern = network_input[start]\r\n",
        "            prediction_output = []\r\n",
        "\r\n",
        "            # generate n_notes\r\n",
        "            for i in range(start, start + n_notes, n_layers):\r\n",
        "                # print(network_input[i:i+n_layers].shape)\r\n",
        "                prediction_input = network_input[i:i+n_layers]\r\n",
        "                # prediction_input = pattern.clone().detach().reshape(1, len(pattern), 1)\r\n",
        "                # prediction_input = prediction_input / float(n_vocab)\r\n",
        "\r\n",
        "                ct_list = self.forward(prediction_input)\r\n",
        "                ct_list = post_processing(ct_list, i)\r\n",
        "                # print(ct_list)\r\n",
        "                # print(\"Gib\")\r\n",
        "                # print(ct_list)\r\n",
        "\r\n",
        "                for prediction in ct_list:\r\n",
        "                  index = torch.mean(prediction)\r\n",
        "                  # print(prediction, index)\r\n",
        "                  result = int_to_note.get(int(index), '0')\r\n",
        "                  prediction_output.append(result)\r\n",
        "\r\n",
        "                # added_index = (index / n_vocab).reshape(1, 1)\r\n",
        "                # pattern = torch.cat((pattern, added_index), 0)\r\n",
        "                # pattern.append(index)\r\n",
        "                # pattern = pattern[1 : len(pattern)]\r\n",
        "\r\n",
        "            return prediction_output"
      ],
      "outputs": [],
      "metadata": {
        "id": "JTxPm8XrDuLa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "n_notes = 20\r\n",
        "req_size = n_notes//n_layers\r\n",
        "start = np.random.randint(0, len(midi.network_input) - req_size)\r\n",
        "with torch.no_grad():\r\n",
        "  prediction_input = midi.network_input[start:start+n_layers]\r\n",
        "  ct_list = lstm.model.predict(prediction_input)\r\n",
        "  print(ct_list[0])"
      ],
      "outputs": [],
      "metadata": {
        "id": "O_b611mvCNtt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print_parameters()"
      ],
      "outputs": [],
      "metadata": {
        "id": "CI1_uUzqBw5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "source": [
        "print(\"Generating notes\")\r\n",
        "notes = generate_notes(\r\n",
        "    lstm, midi.network_input, midi.int_to_note, midi.n_vocab, n_notes=20, n_layers=n_layers\r\n",
        ")\r\n",
        "notes"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating notes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6.7.11',\n",
              " '7.11.0',\n",
              " '308',\n",
              " '308',\n",
              " '308',\n",
              " '9.1.4',\n",
              " 'G#3',\n",
              " '3.5.8',\n",
              " '9',\n",
              " '8.11',\n",
              " '8.10.0',\n",
              " '9.0.2',\n",
              " '11.2',\n",
              " '308',\n",
              " '7.8.10.2',\n",
              " '4',\n",
              " '7.9.11',\n",
              " '5.9.11',\n",
              " '6.9',\n",
              " 'E-6']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "metadata": {
        "id": "q7PVNr3_2cPo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "source": [
        "print(\"Saving as MIDI file.\")\r\n",
        "midi.create_midi_from_model(notes, f\"{model_name}_generated_2.mid\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving as MIDI file.\n"
          ]
        }
      ],
      "metadata": {
        "id": "ChgF6LoQ2cPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbb1f03-51b4-484b-a736-99a9bfad427c"
      }
    }
  ]
}