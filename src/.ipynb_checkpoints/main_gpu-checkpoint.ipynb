{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMjdvyD62otP"
   },
   "outputs": [],
   "source": [
    "!pip install pennylane music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z70RfQb52cPX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7108/1745808544.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import embeddings as emb\n",
    "from pennylane.templates import layers as lay\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import glob, pickle, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y11nHDFJ79-C",
    "outputId": "f7e1c5f8-9594-4206-cfad-988909d5a9a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "uKEIMOeK4VQV",
    "outputId": "054b06bc-6744-4ed4-e7da-ba945530eae4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-e092de28-2d6c-4059-a134-4afd5f3d8b1b\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-e092de28-2d6c-4059-a134-4afd5f3d8b1b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving notes.pk to notes.pk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['notes.pk'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "uploaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nyTkc6uS2cPb"
   },
   "outputs": [],
   "source": [
    "# Midi.py\n",
    "\n",
    "notes_dir = \"notes.pk\"\n",
    "\n",
    "class Midi:\n",
    "    def __init__(self, seq_length, device):\n",
    "        self.seq_length = seq_length\n",
    "        self.device = device\n",
    "\n",
    "        if Path(notes_dir).is_file():\n",
    "            self.notes = pickle.loads(uploaded[notes_dir])\n",
    "        else:\n",
    "            self.notes = self.get_notes()\n",
    "            pickle.dump(self.notes, open(notes_dir, \"wb\"))\n",
    "\n",
    "        self.network_input, self.network_output = self.prepare_sequences(self.notes)\n",
    "        print(f\"Input shape: {self.network_input.shape}\")\n",
    "        print(f\"Output shape: {self.network_output.shape}\")\n",
    "\n",
    "    def get_notes(self):\n",
    "        \"\"\"Get all the notes and chords from the midi files in the ./midi_songs directory\"\"\"\n",
    "        # This is assuming that every interval between notes is the same (0.5)\n",
    "        notes = []\n",
    "\n",
    "        for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "            midi = converter.parse(file)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            notes_to_parse = None\n",
    "\n",
    "            try:  # file has instrument parts\n",
    "                s2 = instrument.partitionByInstrument(midi)\n",
    "                notes_to_parse = s2.parts[0].recurse()\n",
    "            except:  # file has notes in a flat structure\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
    "\n",
    "        with open(notes_dir, \"wb\") as filepath:\n",
    "            pickle.dump(notes, filepath)\n",
    "\n",
    "        return notes\n",
    "\n",
    "    def prepare_sequences(self, notes):\n",
    "        \"\"\"Prepare the sequences used by the Neural Network\"\"\"\n",
    "        self.n_vocab = len(set(notes))\n",
    "\n",
    "        # get all pitch names\n",
    "        pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "        # create a dictionary to map pitches to integers\n",
    "        self.note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
    "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
    "\n",
    "        network_input = []\n",
    "        network_output = []\n",
    "\n",
    "        # create input sequences and the corresponding outputs\n",
    "        for i in range(0, len(self.notes) - self.seq_length, 1):\n",
    "            sequence_in = self.notes[i : i + self.seq_length]\n",
    "            sequence_out = self.notes[i + self.seq_length]\n",
    "            network_input.append([self.note_to_int[char] for char in sequence_in])\n",
    "            network_output.append(self.note_to_int[sequence_out])\n",
    "\n",
    "        n_patterns = len(network_input)\n",
    "\n",
    "        # reshape the input into a format compatible with LSTM layers\n",
    "        # So this is actuallyt (number of different inputs, sequence length, number of features)\n",
    "        network_input = np.reshape(network_input, (n_patterns, self.seq_length, 1))\n",
    "        # normalize input\n",
    "        network_input = network_input / float(self.n_vocab)\n",
    "\n",
    "        # network_output = to_categorical(network_output)\n",
    "\n",
    "        return (tensor(network_input, device=self.device), tensor(network_output, device=self.device))\n",
    "\n",
    "    def create_midi_from_model(self, prediction_output, filename):\n",
    "        \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "        offset = 0\n",
    "        output_notes = []\n",
    "\n",
    "        # create note and chord objects based on the values generated by the model\n",
    "        for pattern in prediction_output:\n",
    "            # pattern is a chord\n",
    "            if ('.' in pattern) or pattern.isdigit():\n",
    "                notes_in_chord = pattern.split('.')\n",
    "                notes = []\n",
    "                for current_note in notes_in_chord:\n",
    "                    new_note = note.Note(int(current_note))\n",
    "                    new_note.storedInstrument = instrument.Piano()\n",
    "                    notes.append(new_note)\n",
    "                new_chord = chord.Chord(notes)\n",
    "                new_chord.offset = offset\n",
    "                output_notes.append(new_chord)\n",
    "            # pattern is a note\n",
    "            else:\n",
    "                new_note = note.Note(pattern)\n",
    "                new_note.offset = offset\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                output_notes.append(new_note)\n",
    "\n",
    "            # increase offset each iteration so that notes do not stack\n",
    "            offset += 0.5\n",
    "\n",
    "        midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "        midi_stream.write('midi', fp=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZIMMZq2j2cPh"
   },
   "outputs": [],
   "source": [
    "# QLSTM.py\n",
    "\n",
    "Embedding = Union[emb.AngleEmbedding, emb.AmplitudeEmbedding, emb.BasisEmbedding]\n",
    "Layer = Union[\n",
    "    lay.BasicEntanglerLayers,\n",
    "    lay.ParticleConservingU1,\n",
    "    lay.ParticleConservingU2,\n",
    "    lay.RandomLayers,\n",
    "    lay.StronglyEntanglingLayers,\n",
    "]\n",
    "\n",
    "\n",
    "class QLSTM(nn.Module):\n",
    "    def quantum_op(\n",
    "        self,\n",
    "        wires,\n",
    "        embedding: Embedding = emb.AngleEmbedding,\n",
    "        layer: Layer = lay.BasicEntanglerLayers,\n",
    "    ):\n",
    "        def circuit_part(inputs, weights):\n",
    "            embedding(inputs, wires=wires)\n",
    "            layer(weights, wires=wires)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in wires]\n",
    "\n",
    "        return circuit_part\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        n_qubits=4,\n",
    "        n_qlayers=1,\n",
    "        batch_first=True,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        backend=\"default.qubit\",\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "        self.device = device # \"cpu\", \"cuda\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "\n",
    "        # self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        # self.dev = qml.device('qiskit.basicaer', wires=self.n_qubits)\n",
    "        # self.dev = qml.device('qiskit.ibm', wires=self.n_qubits)\n",
    "        # use 'qiskit.ibmq' instead to run on hardware\n",
    "\n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "\n",
    "        self.qlayer_forget = qml.QNode(\n",
    "            self.quantum_op(self.wires_forget), self.dev_forget, interface=\"torch\"\n",
    "        )\n",
    "\n",
    "        self.qlayer_input = qml.QNode(\n",
    "            self.quantum_op(self.wires_input), self.dev_input, interface=\"torch\"\n",
    "        )\n",
    "\n",
    "        self.qlayer_update = qml.QNode(\n",
    "            self.quantum_op(self.wires_update), self.dev_update, interface=\"torch\"\n",
    "        )\n",
    "\n",
    "        self.qlayer_output = qml.QNode(\n",
    "            self.quantum_op(self.wires_output), self.dev_output, interface=\"torch\"\n",
    "        )\n",
    "\n",
    "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\n",
    "        self.VQC = {\n",
    "            \"forget\": qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes).to(device),\n",
    "            \"input\": qml.qnn.TorchLayer(self.qlayer_input, weight_shapes).to(device),\n",
    "            \"update\": qml.qnn.TorchLayer(self.qlayer_update, weight_shapes).to(device),\n",
    "            \"output\": qml.qnn.TorchLayer(self.qlayer_output, weight_shapes).to(device),\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        # self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        \"\"\"\n",
    "        # Automatically assumes single batch\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.reshape(1, x.shape[0], x.shape[1])\n",
    "        \n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size, device=self.device) # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size, device=self.device) # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1).float()\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(\n",
    "                self.clayer_out(self.VQC[\"forget\"](y_t).to(self.device))\n",
    "            )  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC[\"input\"](y_t).to(self.device)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC[\"update\"](y_t).to(self.device)))  # update block\n",
    "            o_t = torch.sigmoid(\n",
    "                self.clayer_out(self.VQC[\"output\"](y_t).to(self.device))\n",
    "            )  # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        # Wow, such pseudo-keras!\n",
    "        h_t, c_t = h_t.float(), c_t.float()\n",
    "\n",
    "        if self.return_state:\n",
    "            if self.return_sequences:\n",
    "                return hidden_seq, (h_t, c_t)\n",
    "            else:\n",
    "                return (h_t, c_t)\n",
    "        else:\n",
    "            if self.return_sequences:\n",
    "                return hidden_seq\n",
    "            else:\n",
    "                return h_t\n",
    "    \n",
    "    def predict(self, x, init_states=None):\n",
    "        return self.forward(x, init_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3AgEdPUc2cPk"
   },
   "outputs": [],
   "source": [
    "# LSTMusic.py\n",
    "\n",
    "class LSTMusic(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab=None,\n",
    "        input_dim=1,\n",
    "        hidden_dim=512,\n",
    "        n_qubits=4,\n",
    "        backend=\"default.qubit\",\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(LSTMusic, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        if n_qubits > 0:\n",
    "            print(f\"Generator will use Quantum LSTM running on backend {backend}\")\n",
    "\n",
    "            # self.model = nn.Sequential(\n",
    "            #     QLSTM(\n",
    "            #         input_size=input_dim,\n",
    "            #         hidden_size=hidden_dim,\n",
    "            #         n_qubits=n_qubits,\n",
    "            #         backend=backend,\n",
    "            #     ),\n",
    "            #     nn.Dropout(0.3),\n",
    "            #     QLSTM(\n",
    "            #         input_size=input_dim,\n",
    "            #         hidden_size=hidden_dim,\n",
    "            #         n_qubits=n_qubits,\n",
    "            #         backend=backend,\n",
    "            #     ),\n",
    "            #     nn.Dropout(0.3),\n",
    "            #     QLSTM(\n",
    "            #         input_size=input_dim,\n",
    "            #         hidden_size=hidden_dim,\n",
    "            #         n_qubits=n_qubits,\n",
    "            #         backend=backend,\n",
    "            #     ),\n",
    "            #     nn.Linear(hidden_dim, hidden_dim),\n",
    "            #     nn.Dropout(0.3),\n",
    "            #     nn.Linear(hidden_dim, n_vocab),\n",
    "            # )\n",
    "\n",
    "            self.model = QLSTM(\n",
    "                input_dim,\n",
    "                hidden_dim,\n",
    "                n_qubits=n_qubits,\n",
    "                backend=backend,\n",
    "                return_state=True,\n",
    "                device=device,\n",
    "            ).to(device)\n",
    "        else:\n",
    "            print(\"Generator will use Classical LSTM\")\n",
    "            self.model = nn.LSTM(input_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "\n",
    "    def forward(self, note_sequence):\n",
    "        (h_t, c_t) = self.model(note_sequence)\n",
    "        scores = F.log_softmax(c_t, dim=1)\n",
    "        return scores\n",
    "\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        # lstm_out, _ = self.model(embeds.view(len(sentence), 1, -1))\n",
    "        # tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        # tag_scores = F.log_softmax(tag_logits, dim=1)\n",
    "        # return tag_scores\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        mode=True,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        n_epochs=None,\n",
    "        cutoff: int = None,\n",
    "        learning_rate=0.1,\n",
    "    ):\n",
    "        # Same as categorical cross entropy, who would've thought?!\n",
    "        if mode == False:\n",
    "            return\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        if cutoff:\n",
    "            inputs = inputs[:cutoff]\n",
    "            outputs = outputs[:cutoff]\n",
    "\n",
    "        history = {\"loss\": []}\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            counter = 1\n",
    "            losses = []\n",
    "            for note_series, next_note in zip(inputs, outputs):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Step 2. Run our forward pass.\n",
    "                c_t = self(note_series)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(c_t, next_note.reshape(1).long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(float(loss))\n",
    "\n",
    "                print(f\"On datapoint #{counter} out of {cutoff}\")\n",
    "                counter += 1\n",
    "\n",
    "            avg_loss = np.mean(losses)\n",
    "            history[\"loss\"].append(avg_loss)\n",
    "            print(\"Epoch {} / {}: Loss = {:.3f}\".format(epoch + 1, n_epochs, avg_loss))\n",
    "        return history\n",
    "\n",
    "    def generate_notes(self, network_input, int_to_note, n_vocab, n_notes):\n",
    "        \"\"\"Generate notes from the neural network based on a sequence of notes\"\"\"\n",
    "        # pick a random sequence from the input as a starting point for the prediction\n",
    "        with torch.no_grad():\n",
    "            start = np.random.randint(0, len(network_input) - 1)\n",
    "\n",
    "            pattern = network_input[start]\n",
    "            prediction_output = []\n",
    "\n",
    "            # generate 500 notes\n",
    "            for _ in range(n_notes):\n",
    "                prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "                # prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "                (h_t, prediction) = self.model.predict(prediction_input)\n",
    "\n",
    "                index = np.argmax(prediction)\n",
    "                result = int_to_note[int(index)]\n",
    "                prediction_output.append(result)\n",
    "\n",
    "                added_index = (index / n_vocab).reshape(1, 1)\n",
    "\n",
    "                pattern = torch.cat((pattern, added_index), 0)\n",
    "                # pattern.append(index)\n",
    "                pattern = pattern[1 : len(pattern)]\n",
    "\n",
    "            return prediction_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "j0r6iXZ82cPm"
   },
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "n_epochs = 10\n",
    "cutoff = 1000\n",
    "n_qubits = 8\n",
    "\n",
    "model_name = f\"lstm-seq{seq_length}-cut{cutoff}-epcs{n_epochs}-qu{n_qubits}\"\n",
    "model_str = f\"saved_models/{model_name}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a33Nq6eh2cPm",
    "outputId": "e7ac6cf9-ee9a-4efb-f9e0-ecd106edc5dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Midi\n",
      "Input shape: torch.Size([44831, 25, 1])\n",
      "Output shape: torch.Size([44831])\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialized Midi\")\n",
    "midi = Midi(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "-rkZC4Yb2cPn",
    "outputId": "604582be-2dc1-45c7-c455-2e57e9bc7a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized LSTM\n",
      "Generator will use Quantum LSTM running on backend default.qubit\n",
      "weight_shapes = (n_qlayers, n_qubits) = (1, 8)\n",
      "Training LSTM\n",
      "On datapoint #0 out of 1000, taking time 13.260213136672974\n",
      "On datapoint #1 out of 1000, taking time 12.862241268157959\n",
      "On datapoint #2 out of 1000, taking time 13.108321189880371\n",
      "On datapoint #3 out of 1000, taking time 12.859251022338867\n",
      "On datapoint #4 out of 1000, taking time 12.853482007980347\n",
      "On datapoint #5 out of 1000, taking time 13.141092538833618\n",
      "On datapoint #6 out of 1000, taking time 12.7720205783844\n",
      "On datapoint #7 out of 1000, taking time 13.313769340515137\n",
      "On datapoint #8 out of 1000, taking time 12.9338219165802\n",
      "On datapoint #9 out of 1000, taking time 13.235898494720459\n",
      "On datapoint #10 out of 1000, taking time 12.914175033569336\n",
      "On datapoint #11 out of 1000, taking time 12.955796957015991\n",
      "On datapoint #12 out of 1000, taking time 13.145618915557861\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-88ee6f2a471d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training LSTM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     train_history = lstm.train(\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-1a0904a4729d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode, inputs, outputs, n_epochs, cutoff, learning_rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;31m#  calling optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_note\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Initialized LSTM\")\n",
    "lstm = LSTMusic(hidden_dim=midi.n_vocab, n_qubits=n_qubits).to(device)\n",
    "\n",
    "if Path(model_str).is_file():\n",
    "    print(\"Loading model\")\n",
    "    lstm.load_state_dict(torch.load(model_str))\n",
    "    lstm.eval()\n",
    "    # lstm = torch.load(model_str)\n",
    "else:\n",
    "    print(\"Training LSTM\")\n",
    "    train_history = lstm.train(\n",
    "        True, midi.network_input, midi.network_output, n_epochs=n_epochs, cutoff=cutoff\n",
    "    )\n",
    "    torch.save(lstm.state_dict(), model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7PVNr3_2cPo"
   },
   "outputs": [],
   "source": [
    "print(\"Generating notes\")\n",
    "notes = lstm.generate_notes(\n",
    "    midi.network_input, midi.int_to_note, midi.n_vocab, n_notes=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChgF6LoQ2cPq"
   },
   "outputs": [],
   "source": [
    "print(\"Saving as MIDI file.\")\n",
    "midi.create_midi_from_model(notes, f\"generated_songs/{model_name}_generated.mid\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "main_gpu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
